\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{II Stochastic Financial Models}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2023

		\vspace{1.5em}

		\Large

		Based on Lectures by Dr. Michael Tehranchi

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

\section{Introduction}
\label{sec:intro}

Probability and Measure is desirable; not required. We will talk about about $\sigma$-algebras later. Moreover sometimes Fubini's or the dominated convergence theorem is used, without proof.

In the following models, we will use the following assumptions. Note that these are clearly false in the real world:

\begin{itemize}
	\item No dividends.
	\item  Shares are infinitely divisible. This is a reasonable assumption for a large investor.
	\item No bid-ask spread, which is reasonable for liquid markets.
	\item No price impact of buying/selling, so our investing size is not too large.
	\item No transaction fees.
	\item No short-selling constraints.
\end{itemize}

In the following, we use the conventions:
\begin{itemize}
	\item We will have $d$ stocks (risky assets).
	\item $S^i_t$ is the price of asset $i$ at time $t$. For this course, $t \in \{0, 1\}$ or $\{0, 1, 2, \ldots\}$ or $[0,\infty]$. The first few lectures will be focused on the one-period model.
	\item $S_t = (S^1_t, \ldots, S^d_t)^T \in \mathbb{R}^d$ is the vector of asset prices.
	\item We will also assume there is a constant risk-free interest rate $r$.
\end{itemize}

\subsection{One-period Setup}
\label{sub:one_period_setup}

For one-period, we will assume that $S_0$ is not random, but $S_1$ is a random vector. Moreover will assume that there is no uncertainty in the distribution.

Our investor holds $\theta^i$ shares of asset $i$. Denote by $\theta = (\theta^1, \ldots, \theta^d)^T \in \mathbb{R}^d$ the vector of the total position of the investor.

Note that $\theta^i < 0$ means that the investor is shorting $|\theta^i|$ shares.

The investor will also put $\theta^0$ in the bank, gaining the risk-free interest rate on this investment.

Let $X_t$ denote the wealth, or liquidation value, of the portfolio at time $t$. At time $0$, this is simply
\[
X_0 = \theta^0 + \sum_{i=1}^{d} \theta^i S^i_0 = \theta^0 + \theta \cdot S_0.
\]
As time progresses, $\theta^0$ accrues interest, so
\[
X_1 = \theta^0 (1+r) + \theta \cdot S_1.
\]
Hence
\[
	X_1 = (1+r)X_0 + \theta \cdot (S_1 - (1+r) S_0). \tag{1.1}\label{eq:1.1}
\]
In general,
\[
	X_t = (1+r)X_{t-1} + \theta_t \cdot (S_t - (1+r)S_{t-1}). \tag{1.2}\label{eq:1.2}
\]

\newpage

\section{The Mean-Variance Portfolio Problem}
\label{sec:mv_problem}

This is a problem first proposed by Markowitz in 1952. There are several variations; sometimes we want to find solutions where $\theta^i \geq 0$ for all $i$, i.e. we are not allowed to short.

Given $X_0$, our goal is to find the portfolio $\theta \in \mathbb{R}^{d}$ to minimise $\Var(X_1)$, subject to $\mathbb{E}[X_1] \ge m$.

We will say that $\mathbb{E}[S_1] = \mu \in \mathbb{R}^{d}$, and $\Cov(S_1) = \mathbb{E}[(S_1 - \mu)(S_1 - \mu)^T] = V$. This covariance matrix is symmetric and non-negative definite.

For this problem assume that $\mu \neq (1+r)S_0$, and that $V$ is positive definite, hence $V^{-1}$ exists. Then, from (\ref{eq:1.1}) we get
\[
\mathbb{E}[X_1] = (1+r)X_0 + \theta \cdot (\mu - (1+r)S_0),
\]
\[
\Var(X_1) = \theta^{T} V \theta.
\]
Our problem is hence to minimize $\theta^T V \theta$ subject to $\theta^T (\mu - (1+r)S_0) \ge m - (1+r)X_0$.

We will not prove this, but note the following:

\begin{theorem}
	The optimal portfolio is
	\[
	\theta = \lambda V^{-1}(\mu - (1+r)S_0),
	\]
	where
	\[
	\lambda = \frac{(m-(1+r)X_0))^+}{(\mu - (1+r)S_0)^T V^{-1} (\mu - (1+r)S_0)}.
	\]
\end{theorem}

Here we use the convention $a^{+} = \max\{a, 0\}$.

%lecture 2

\begin{lemma}
	The minima of $\theta^{T}V\theta$ subject to $\theta^{T}a = b$ is
	\[
	\frac{b^2}{a^{T}V^{-1}a},
	\]
	where $V$ is positive definite, $a \in \mathbb{R}^{d}$, $a \neq 0$ and $b \in \mathbb{R}$ is given. The unique minimiser is $\theta = \lambda V^{-1} a$, where
	\[
	\lambda = \frac{b}{a^{T}V^{-1}a}.
	\]
\end{lemma}

\begin{proofbox}
	The first method will be taking the Lagrangian
	\[
	L(\theta, \lambda) = \frac{1}{2} \theta^{T} V\theta + \lambda(b - a^{T}\theta).
	\]
	We can minimize by setting the derivatives $0$, i.e.
	\[
	D_\theta L = V\theta - \lambda a = 0 \implies \theta = \lambda V^{-1} a.
	\]
	Pick $\lambda$ to be feasible: so
	\[
	\theta^{T} a = b \implies \lambda a^{T} V^{-1} a = b \implies \lambda = \frac{b}{a^{T}V^{-1}a}.
	\]
	This isn't very formal; let's work backwards instead. Define $\lambda = b/(a^{T}V^{-1}a)$, and suppose $\theta^{T}a = b$. Then we get
	\begin{align*}
		\theta^{T}V\theta &= \theta^{T}V\theta + 2\lambda(b - a^{T}\theta) \\
				  &= (\theta - \lambda V^{-1}a)^{T}V(\theta - \lambda V^{-1}a) + 2 \lambda b - \lambda^2 a^{T}V^{-1} a \\
				  &\geq 2 \lambda b - \lambda^2 a^{T} V^{-1} a = \frac{b^2}{a^{T}V^{-1}a}.
	\end{align*}
	Since $V$ is positive definite, we have equality if and only if $\theta = \lambda V^{-1}a$.

	Let's look at another proof. Note that
	\begin{align*}
		(\theta^{T}a)^2 &= ((V^{1/2}\theta)^{T}(V^{-1/2}a))^2 \\
				&\leq (\theta^{T}V\theta)(a^{T}V^{-1}a) \\
		\implies \theta^{T}V\theta &\geq \frac{(a^{T}\theta)^2}{a^{T}V^{-1}a} = \frac{b^2}{a^{T}V^{-1}a},
	\end{align*}
	when $a^{T}\theta = b$. We have equality in Cauchy-Schwarz if and only if $V^{1/2}\theta = \lambda V^{-1/2}a$ for some $\lambda \in \mathbb{R}$, or $\theta = \lambda V^{-1}a$.
\end{proofbox}

As an application, if $a = \mu - (1 + r)S_0$, and $b = \mathbb{E}[X_1] - (1+r)X_0$, then
\[
\Var(X_1) \geq \frac{(\mathbb{E}[X_1] - (1+r)X_0)^2}{(\mu - (1+r)S_0)^{T}V^{-1}(\mu - (1+r)S_0)},
\]
with equality if and only if $\theta = \lambda V^{-1}(\mu - (1+r)S_0)$, where
\[
\lambda = \frac{\mathbb{E}[X_1] - (1+r)X_0}{(\mu - (1+r)S_0)^{T}V^{-1}(\mu - (1+r)S_0)}.
\]

\begin{definition}
	The \emph{mean-variance efficient frontier}\index{mean-variance efficient frontier} is the lower boundary of the set of possible pairs $(\mathbb{E}[X_1], \Var(X_1))$, i.e. the points on the parabola.

	A portfolio is \emph{mean-variance efficient}\index{mean-variance efficient} if it is the optimal solution of some mean-variance portfolio problem.
\end{definition}

Recall that we wanted to minimize $\Var(X_1)$ subject to $\mathbb{E}[X_1] \geq m$. However we can figure out what do to by looking at the above. If $m \leq (1+r)X_0$ then we choose $\theta = 0$ (i.e. we don't invest in risky assets). If $m \geq (1+r)X_0$, we choose $\theta = \lambda V^{-1} (\mu - (1+r)S_0)$, where $\lambda$ is as before.

\begin{theorem}[Mutual fund theorem]\index{mutual fund theorem}
	$\theta$ is mean-variance efficient if and only if $\theta = \lambda V^{-1}(\mu - (1+r)S_0)$ for some $\lambda \geq 0$.
\end{theorem}

\begin{proofbox}
	If it is mean-variance efficient, we know it must have this form. Conversely, given $\lambda \geq 0$, let $m = (1+r)X_0 + \lambda(\mu - (1+r)S_0)^{T} V^{-1} (\mu - (1+r)S_0)$.  Then $\theta$ is the optimal solution to the mean-variance problem with target $m$.
\end{proofbox}

\subsection{Capital Asset Pricing Model}
\label{sub:capm}

This subsection will be an application of the mutual fund theorem.

\begin{theorem}[Linear regression coefficients]
	Let $X$ and $Y$ be square-integrable, with $\Var(X) > 0$. There are unique constants $a$ and $b$ such that
	\[
	Y = a + bX + Z,
	\]
	where $\mathbb{E}[Z] = 0$, and $\Cov(X, Z) = 0$.
\end{theorem}

\begin{proofbox}
	Suppose this formula holds, so $Z = Y - a - bX$. Then
	\[
	\mathbb{E}[Z] = \mathbb{E}[Y] - a - b \mathbb{E}[X],
	\]
	\[
	\Cov(X, Z) = \Cov(X, Y) - b \Var(X).
	\]
	These are two unknowns, so their unique solution to $\mathbb{E}[Z] = 0$ and $\Cov(X, Z) = 0$ is given by
	\[
	b = \frac{\Cov(X, Y)}{\Var(X)}, \qquad a = \mathbb{E}[Y] - b \mathbb{E}[X].
	\]
\end{proofbox}

\begin{definition}
	$\theta_{\mathrm{mar}} = V^{-1}(\mu - (1+r)S_0)$ is called the \emph{market portfolio}\index{market portfolio}.
\end{definition}

\begin{definition}
	Fix $X_0 > 0$ and $\theta \in \mathbb{R}^{d}$. The \emph{excess return of the portfolio}\index{excess return} $R^{\mathrm{ex}}$ is
	\[
	\frac{X_1}{X_0} - (1+r) = \frac{\theta^{T}}{X_0}(S_1 - (1+r)S_0).
	\]
	Then the excess return of the market is
	\[
		R^{\mathrm{ex}}_{\mathrm{mar}} = \frac{\theta^{T}_{\mathrm{mar}}}{X_0}(S_1 - (1+r)S_0).
	\]
\end{definition}

\begin{theorem}[$\alpha$ is zero]
	Fix $X_0 > 0$ and $\theta \in \mathbb{R}^{d}$. Let $\alpha, \beta$ be such that
	\[
		R^{\mathrm{ex}} = \alpha + \beta R^{\mathrm{ex}}_{\mathrm{mar}} + \eps,
	\]
	where $\mathbb{E}[\eps] = 0$ and $\Cov(\eps, R^{\mathrm{ex}}_{\mathrm{mar}}) = 0$. Then $\alpha = 0$.
\end{theorem}

%lecture 3

\begin{proofbox}
	This is just a calculation. We need to calculate
	\begin{align*}
	\Cov(R^{\mathrm{ex}}, R^{\mathrm{ex}}_{\mathrm{mar}}) &= \Cov \biggl( \frac{\theta^{T}}{X_0}(S_1 - (1+r)S_0), \frac{\theta^{T}_{\mathrm{mar}}}{X_0}(S_1-(1+r)S_0)\biggr) \\
											 &= \frac{\theta^{T}}{X_0}\frac{V \theta_{\mathrm{mar}}}{X_0} = \frac{\theta^{T}}{X_0^2} (\mu - (1+r)S_0) \\
											 &= \frac{\mathbb{E}[R^{\mathrm{ex}}]}{X_0}.
	\end{align*}
	Hence we get
	\[
	\Var(R^{\mathrm{ex}}_{\mathrm{max}}) = \frac{\mathbb{E}[R^{\mathrm{ex}}_{\mathrm{mar}}]}{X_0} = \Cov(R^{\mathrm{ex}}_{\mathrm{mar}}, R^{\mathrm{ex}}_{\mathrm{mar}}).
	\]
	Therefore
	\[
	\beta = \frac{\Cov(R^{\mathrm{ex}}, R^{\mathrm{ex}}_{\mathrm{mar}})}{\Var(R^{\mathrm{ex}}_{\mathrm{mar}})} = \frac{\mathbb{E}[R^{\mathrm{ex}}]}{\mathbb{E}[R^{\mathrm{ex}}_{\mathrm{mar}}]},
	\]
	and so
	\[
	\alpha = \mathbb{E}[R^{\mathrm{ex}}] - \beta \mathbb{E}[R^{\mathrm{ex}}_{\mathrm{mar}}] = 0.
	\]
\end{proofbox}

The assumptions in our model are as follows:
\begin{itemize}
	\item There are $K$ agents, where agent $k$ holds portfolio $\theta_k$.
	\item There are $n_i$ shares of asset $i$ in total. Let $n = (n_1, \ldots, n_d)^{T} > 0$.
	\item All investors are mean-variance efficient, and they agree on the mean and covariance of $S_1$.
	\item By mutual fund theorem $\theta_k = \lambda_k \theta_{\mathrm{mar}}$ for $\lambda_k \geq 0$ scalar.
	\item As supply equals demand,
		\[
		\sum_{k = 1}^{K}\theta_k = n = \Biggl( \sum_{k = 1}^{K} \lambda_k \Biggr) \theta_{\mathrm{mar}} = \Lambda \theta_{\mathrm{mar}}.
		\]
\end{itemize}

The implication is that $R^{\mathrm{ex}}_{\mathrm{mar}}$ is the excess return of a broad market index (such as S\&P 500 or FTSE), and that
\[
R^{\mathrm{ex}} = \beta R^{\mathrm{ex}}_{\mathrm{mar}} + \eps.
\]
This is a testable theory, and we can test it in the market and see that it often fails; we can find (negative) $\alpha$ in places. The reason is that some of the assumptions we have made are very strong: we assume a frictionless market without spreads and market impact, but we also assume that all other investors behave nicely.

\newpage

\section{Expected Utility Hypothesis}
\label{sec:utility}

So far, implicitly, an investor would prefer $X$ over $Y$ if either:
\begin{itemize}
	\item $\mathbb{E}[X] > \mathbb{E}[Y]$, $\Var(X) \leq \Var(Y)$, or
	\item $\mathbb{E}[X] \geq \mathbb{E}[Y]$, $\Var(X) < \Var(Y)$.
\end{itemize}

This is quite strong. Let's assume a different condition.

\begin{proposition}[Expected Utility Hypothesis]\index{expected utility hypothesis}\index{utility function}
	An agent prefers $X$ to $Y$ if and only if
	 \[
	\mathbb{E}[U(X)] \geq \mathbb{E}[U(Y)],
	\]
	for a given function $U$, called the agent's utility function.
\end{proposition}

Note that if $\hat U(x) = a + bU(x)$ for all $x$, where $a, b \in \mathbb{R}$ and $b > 0$, then the preferences induced by $\hat U$ and $U$ are the same. The von-Neumann-Morgenstern theorem characterizes the preferences equivalent to the expected utility hypothesis.

\begin{definition}
	An expected utility agent is \emph{indifferent}\index{indifferent} to $X$ and $Y$ if and only if $\mathbb{E}[U(X)] = \mathbb{E}[U(Y)]$.
\end{definition}

\subsection{Risk-aversion and Concavity}

For now, we will assume:
\begin{itemize}
	\item $U$ is strictly increasing,
		 \[
		U(x) > U(y) \iff x > y.
		\]
		The implication is, if $X \geq Y$ a.s. and $\mathbb{P}(X > Y) > 0$, then $\mathbb{E}[U(X)] > \mathbb{E}[U(Y)]$, so $X$ is preferred to $Y$.
	\item $U$ is concave,
		\[
		U(px + (1-p)y) > pU(x) + (1-p)U(y),
		\]
		for all $x \neq y$ and $0 < p < 1$.

		The implication is, if $X$ is not constant, then by Jensen's inequality
		\[
		\mathbb{E}[U(X)] < U(\mathbb{E}[X]).
		\]
		Hence $\mathbb{E}[X]$ is preferred to $X$.
\end{itemize}

%lecture 4

\subsection{Concave functions}
\label{sub:concave}

First we assume that $U$ is always increasing, strictly concave and twice differentiable.

The derivative $U'$ is called the \emph{marginal utility}\index{marginal utility}. This is positive as $U$ is increasing, but decreasing as $U$ is concave. This means that $U''$ is negative.

\begin{definition}
	The \emph{coefficient of absolute risk aversion}\index{absolute risk aversion} is
	\[
	\frac{-U''(x)}{U'(x)}.
	\]
	The \emph{coefficient of relative risk aversion}\index{relative risk aversion} is
	\[
	\frac{- x U''(x)}{U'(x)},
	\]
	when $x > 0$.
\end{definition}

\begin{exbox}
	Here are some important utility functions.
	\begin{enumerate}
		\item $U(x) = - e^{-\gamma x}$ is the CARA (constant absolute risk aversion) utility function. Indeed,
			\[
			U'(x) = \gamma e^{-\gamma x}, \qquad U''(x) = - \gamma^2 e^{-\gamma x},
			\]
			\[
			\implies \frac{U''(x)}{U'(x)} = \gamma
			\]
			is constant.
	\item $U(x) = x^{1-R}/(1-R)$, for $R > 0$, $R \neq 1$ is the CRRA (constant relative risk aversion) utility function. We can calculate that
		\[
		\frac{-x U''(x)}{U'(x)} = R.
		\]
		Note this function is not defined everywhere. To be fully define the functions, we can let
		\[
		U(x)=
		\begin{cases}
			\frac{x^{1-R}}{1-R} & x > 0,\\
			-\infty & x \leq 0.
		\end{cases}
		\]
		\item $U(x) = \log x$ is the CRRA utility function with $R = 1$.
		\item $U(x) = x$ is a utility function that is not strictly concave. This is the \emph{risk-neutral utility}.
	\end{enumerate}
\end{exbox}

Let's prove some basic facts about concave functions.

\begin{theorem}
	Concave functions are continuous, and differential concave functions have graphs that lie beneath their tangents.
\end{theorem}

To prove this we first prove a lemma:

\begin{lemma}
	Let $U$ be concave and $a < b < c$. Then,
	\[
	\frac{u(b) - u(a)}{b - a} \geq \frac{u(c) - u(b)}{c - b}.
	\]
\end{lemma}

\begin{proofbox}
	Note that
	\[
	b = \biggl( \frac{b-a}{c-a} \biggr) c + \biggl(\frac{c-b}{c-a} \biggr)a,
	\]
	and these coefficients are in $(0, 1)$. Hence applying the definition of concavity,
	\[
		u(b) \geq \biggl( \frac{b-a}{c-a} \biggr) u(c) + \biggl( \frac{c-b}{c-a} \biggr) u(a).
	\]
	Rearranging gives what we want.
\end{proofbox}

Now we can apply this lemma to prove continuity.

\begin{proofbox}
	Fix $x$, $0 < \eps < t$. Apply the lemma to
	\[
	x-t < x - \eps < x < x + \eps < x + t.
	\]
	Then we get that
	\[
	\frac{\eps}{t} (u(x) - u(x-t)) \geq u(x) - u(x-\eps) \geq u(x+\eps)-u(x) \geq \frac{\eps}{t} (u(x+t) - u(x)),
	\]
	which goes to $0$ as $\eps \to 0$.

	Moreover if $u$ is differentiable at $x$, then we have
	\[
	u(x) - u(x-t) \geq t u'(x) \geq u(x+t) - u(x),
	\]
	by taking $\eps \to 0$. This gives
	\[
	u(y) \leq u(x) + u'(x) (y-x).
	\]
	Note if $u$ is twice differentiable, we can use second-order Taylor's:
	\[
	u(y) = u(x) + u'(x) (y-x) + u''(z) \frac{(y-x)^2}{2},
	\]
	where $u''(z) \leq 0$.
\end{proofbox}

\begin{theorem}
	Increasing concave functions are unbounded to the left, i.e. $u(x) \to -\infty$ as $x \to -\infty$.
\end{theorem}

\begin{proofbox}
	Pick $x < a < b$. Then,
	\[
	u(x) \leq u(a) + \biggl( \frac{x-a}{b-a} \biggr) (u(b) - u(a)),
	\]
	which goes to $-\infty$ as $x \to -\infty$
\end{proofbox}

\subsection{Optimal Investment and Marginal Utility}
\label{sub:optimal_invest}

\begin{theorem}
	Consider the problem of maximizing $\mathbb{E}[U(X_1)]$ given $X_0$, where
	\[
		X_1 = (1+r)X_0 + \theta^{T}(S_1 - (1+r)S_0),
	\]
	where $U$ is increasing, concave and suitable. If $\theta^{\ast}$ is optimal, then
	\[
	S_0 = \frac{1}{1+r} \frac{\mathbb{E}[U'(X_1^{\ast})S_1]}{\mathbb{E}[U'(X_1^{\ast})]},
	\]
	where
	\[
	X_1^{\ast} = (1+r)X_0 + (\theta^{\ast})^{T}(S_1 - (1+r)S_0).
	\]
\end{theorem}

%lecture 5

\begin{proofbox}
	Let 
	\[
	f(\theta) = \mathbb{E}[U((1+r)X_0) + \theta^{T}(S_1-(1+r)S_0))].
	\]
	At the maximum,
	\[
	Df(\theta^{\ast}) = 0 = \mathbb{E}[U'(X_1^{\ast})(S_1-(1+r)S_0)].
	\]
	This rearranges to what we want.
\end{proofbox}

\newpage

\section{Pricing Contingent Claims}
\label{sec:price_cont}

For us, a contingent claim is a random variable (the payout is random). The payout for us is at time 1. We will introduce the following framework:
\begin{itemize}
	\item There are $d$ assets given (fundamental).
	\item We introduce a new contingent claim with payout $Y$.
	\item Often $Y = g(S_1)$, a function of the price of the stocks at time 1, but not necessarily.
\end{itemize}
We are interested in finding a good time $0$ price of the claim.

The most important example is a \emph{call option}\index{call option}: the right but not the obligation to buy some given asset at a fixed price $K$, often called the \emph{strike price}\index{strike price}.

We will assume that the actors are rational. That is, if at time 1 the asset has price greater than $K$, then the buyer will exercise the option and sell the asset on the market, making $S_1 - K$. If the asset has price less than $K$, then the buyer will not exercise the right. This gives a payout
\begin{align*}
	\text{payout} &= 
	\begin{cases}
		S_1 - K & S_1 > K,\\
		0 & S_1 \leq K,
	\end{cases}
	\\
		      &= (S_1-K)^{+} = g(S_1).
\end{align*}

\subsection{Utility Indifference Pricing}
\label{sub:u_price}

Fix $X_0$ to be an initial wealth, and $U$ an increasing concave utility function. Let
\[
	\mathcal{X} = \{X_1 = (1+r)X_0 + \theta^{T}(S_1 - (1+r)S_0) \mid \theta \in \mathbb{R}^{d} \}.
\]
Note that the investor would prefer to buy one claim with payout $Y$ and price $\pi$ if there exists $X_1^{\ast} = X^{\ast}$ such that
\[
\mathbb{E}[U(X^{\ast} + Y - (1+r)\pi)] > \mathbb{E}[U(X)],
\]
for all $X \in \mathcal{X}$.

We will assume $U$, the distribution of $S_1$, $X_0$, $Y$, the values $r, S_0$ and $\pi$ are such that every utility maximisation problem has an optimal solution.

\begin{definition}
	The (utility) \emph{indifference (reservation) price}\index{indifference price} of the claim with payout $Y$ is any $\pi$ such that
	\[
	\max_{X \in \mathcal{X}}\mathbb{E}[U(X+Y-(1+r)\pi)] = \max_{X \in X} \mathbb{E}[U(X)].
	\]
\end{definition}

Here are some properties of the indifference price. First, our assumptions lead to the following.

\begin{theorem}
	The indifference price exists and is unique.
\end{theorem}

From this, we let $\pi(Y)$ denote the indifference price of the claim with payout $Y$. Now we will relate indifference prices of different claims.

\begin{theorem}
	If $Y_0 \leq Y_1$ almost surely and $\mathbb{P}(Y_0 < Y_1) > 0$, then $\pi(Y_0) < \pi(Y_1)$.
\end{theorem}

Moreover, we can show that indifference prices are concave:
\begin{theorem}[Indifference prices are concave]
	\[
	\pi(pY_1 + (1-p)Y_0) \geq p \pi(Y_1) + (1-p)\pi(Y_0),
	\]
	for all payouts $Y_0, Y_1$ and all $0 < p < 1$.
\end{theorem}

\begin{definition}
	The \emph{marginal utility price}\index{marginal utility price} of $Y$ is
	\[
	\pi_0(Y) = \frac{\mathbb{E}[U'(X^{\ast})Y]}{(1+r)\mathbb{E}[U'(X^{\ast})]},
	\]
	where $X^{\ast}$ solves
	\[
	\mathbb{E}[U(X^{\ast})] = \max_{X \in \mathcal{X}} \mathbb{E}[U(X)].
	\]
\end{definition}

Now we have a nice little theorem.

\begin{theorem}
	$\pi(Y) \leq \pi_0(Y)$ for any claim.
\end{theorem}

In fact, we can find the following.

\begin{theorem}
	\[
	\lim_{\eps \to 0} \frac{\pi(\eps Y)}{\eps} = \pi_0(Y).
	\]
\end{theorem}

%lecture 6

To prove this, we introduce the following definition:
\begin{definition}
	We define
	\[
	V(Z) = \max_{X \in \mathcal{X}} \mathbb{E}[U(X+Z)].
	\]
	This is the \emph{indirect utility}\index{indirect utility} of the random payout $Z$.
\end{definition}

In this notation, $\pi$ is the indifference price of payout $Y$ if and only if
\[
V(Y - (1 + r)\pi) = V(0).
\]

\begin{lemma}
	If $Z_0 \leq Z_1$ almost surely, and $\mathbb{P}(Z_0 < Z_1) > 0$, then $V(Z_0) < V(Z_1)$.
\end{lemma}

\begin{proofbox}
	Let $X^i$ be such that
	\[
	V(Z_i) = \mathbb{E}[U(X^i + Z_i)],
	\]
	for $i = 0, 1$. Then
	\begin{align*}
		V(Z_1) &= \mathbb{E}[U(X^1 + Z_1)] \geq \mathbb{E}[U(X^0 + Z_1)] \text{ as $X^1$ is a maximizer}\\
		       &> \mathbb{E}[U(X^0 + Z_0)] \text{ as  } Z_1 \geq Z_0 \\
		       &= V(Z_0).
	\end{align*}
\end{proofbox}

\begin{lemma}
	The indirect utility satisfies
	\[
	V(pZ_1 + (1-p)Z_0) \geq p V(Z_1) + (1-p)Z_0,
	\]
	for all $0 < p < 1$.
\end{lemma}

\begin{proofbox}
	Let $X^i$ be as before. Note that
	\begin{align*}
		p X^1 + (1-p) X^0 &= p((1 + r)X_0 + \theta_1^{T}(S_1 - (1+r)S_0)) \\
				  &\qquad\qquad+ (1-p)((1+r)X_0 + \theta_0^{T}(S_1 - (1+r)S_0)) \\
		&= (1+r)X_0 + (p \theta_1 + (1-p)\theta_0)^{T}(S_1 - (1+r)S_0) \in \mathcal{X},
	\end{align*}
	so we have
	\begin{align*}
		p V(Z_1) + (1-p) V(Z_0) &= \mathbb{E}[p U(X^1 + Z_1) + (1-p)U(X^0 + Z_0)] \\
					&\leq \mathbb{E}[U(pX^1 + (1-p)X^0 + p Z_1 + (1-p) Z_0)] \\
					&\leq \max_{X \in \mathcal{X}} \mathbb{E}[U(X + p Z_1 + (1-p) Z_0)] \\
					&= V(pZ_1 + (1-p)Z_0).
	\end{align*}
\end{proofbox}

Now we can prove the existence and uniqueness of indifference prices.

\begin{proofbox}
	We show there is a unique solution to $V(Y - (1+r)\pi) = V(0)$. Note that
	\[
	V(0) = \mathbb{E}[U(X^{\ast})] \in (U(-\infty), U(\infty)) = (-\infty, U(\infty)).
	\]
	Now consider $x \mapsto V(Y + x)$. This is strictly increasing and concave, by our lemma. Hence it is continuous (by concavity), with range $(\infty, \lim V(Y+x))$.

	But by MCT (actually not entirely sure, Fatou and MCT require non-negativity),
	\[
	V(Y + x) \geq \mathbb{E}[U(X^\ast + Y + x)] \uparrow U(\infty).
	\]
	Hence there exists a unique $x$ such that $V(Y + x) = V(0)$, and the unique $\pi$ is $-x/(1+r)$.
\end{proofbox}

Now we can prove that indifference prices increase.

\begin{proofbox}
	Let $Y_0 \leq Y_1$ almost surely, and $\mathbb{P}(Y_0 < Y_1) > 0$. Then
	\begin{align*}
		V(Y_0 - (1+r)\pi(Y_0)) &= V(0) = V(Y_1 - (1+r) \pi(Y_1)) \\
				       & \geq V(Y_0 - (1+r)\pi(Y_1)),
	\end{align*}
	and so $-(1+r)\pi(Y_0) > -(1+r)\pi(Y_1)$ since $x \mapsto V(Y_0 + x)$ is strictly increasing. Hence $\pi(Y_1) > \pi(Y_0)$.
\end{proofbox}

Here is a proof of concavity.

\begin{proofbox}
	We are given $Y_0, Y_1$ and $0 < p < 1$. Then
	\begin{align*}
		V(pY_1 + (1&-p)Y_0 - (1+r)\pi(pY_1 + (1-p)Y_0)) = V(0) \\
				    &= V(Y_1 - (1+r)\pi(Y_1)) = V(Y_0 - (1+r)\pi(Y_0)) \\
							       &= p V(Y_1 - (1+r)\pi(Y_1)) + (1-p) V(Y_0 - (1+r)\pi(Y_0)) \\
							       &\leq V(pY_1 + (1-p)Y_0 - (1+r)(p \pi(Y_1) + (1-p) \pi(Y_0))),
	\end{align*}
	hence $\pi(pY_1 + (1-p)Y_0) \geq p\pi(Y_1) + (1-p)\pi(Y_0)$.
\end{proofbox}

Now we prove the marginal utility price is bigger than the indifference price.

\begin{proofbox}
	Let $X^{\ast}$ be such that $V(0) = \mathbb{E}[U(X^{\ast})]$. Let $X^1$ be such that $V(Y - (1+r)\pi(Y)) = \mathbb{E}[U(X^1 + Y - (1+r)\pi(Y_0))]$. Then,
	\begin{align*}
		V(0) &= V(Y - (1+r)\pi(Y)) = \mathbb{E}[U(X^1 + Y - (1+r)\pi(Y))] \\
		     &\leq \mathbb{E}[U(X^{\ast}) + U'(X^{\ast})(X^1 - X^{\ast} + Y - (1+r)\pi(Y))] \\
		     &= V(0) + \mathbb{E}[U'(X^{\ast})(X^1 - X^{\ast})] + \mathbb{E}[U'(X^{\ast})(Y - (1+r)\pi(Y))].
	\end{align*}
	Now we claim the bit in the middle is 0. Assuming this, we have
	\[
	\pi(Y) \leq \frac{1}{1 + r} \frac{\mathbb{E}[U'(X^{\ast})Y]}{\mathbb{E}[U'(X^{\ast})]} = \pi_0(Y).
	\]
	To prove this claim note that
	\begin{align*}
		\mathbb{E}[U'(X^{\ast})(X^{\ast} - X^1)] &= (\theta^{\ast} - \theta^1)^{T} \mathbb{E}[U'(X^{\ast}) (S_1 - (1+r)S_0)] = 0,
	\end{align*}
	by the first marginal utility pricing result.
\end{proofbox}

%lecture 7

We now prove the convergence of the indifference price to the marginal utility price.

\begin{proofbox}
	Fix $Y$, and let $\pi_t = \pi(tY)/t$, the average indifference price of $t$ shares. Let
	\[
	p= \sup_{t > 0} \pi_t = \lim_{t \to 0} \pi_t,
	\]
	from the example sheet, where we show $t \mapsto \pi_t$ is decreasing. This is done by using concavity of $\pi$, and the fact $\pi(0) = 0$. Let $X^{\ast} \in \mathcal{X}$ maximize $\mathbb{E}[U(X)]$, as before. Then
	\begin{align*}
		0 &= \frac{V(tY - (1+r)t\pi_t) - V(0)}{t} \\
		  &\geq \frac{1}{t} \mathbb{E}[U(X^{\ast} + t(Y - (1+r)\pi_t)) - U(X^{\ast})] \\
		  &\geq \mathbb{E}\biggl[ \frac{U(X^{\ast} + t(Y - (1+r)p)) - U(X^{\ast})}{t} \biggr] \\
		  &\to \mathbb{E}[U'(X^{\ast})(Y - (1+r)p)],
	\end{align*}
	hence we get
	\[
	p \geq \pi_0 = \frac{\mathbb{E}[U'(X^{\ast})Y]}{(1+r)\mathbb{E}[U'(X^{\ast})]}.
	\]
	But on the other hand,
	\[
	\pi_t \leq \pi_0 \implies p \leq \pi_0,
	\]
	hence $p = \pi_0$.
\end{proofbox}

\newpage

\section{Risk Neutral Measures}
\label{sec:risk_0}

\begin{definition}
	Consider two probability measures $\mathbb{P}$ and $\mathbb{Q}$, defined on a $\sigma$-algebra $\mathcal{F}$.

	Then $\mathbb{P}$ and $\mathbb{Q}$ are equivalent if and only if $\mathbb{P}(A) = 0 \iff \mathbb{Q}(A) = 0$.
\end{definition}

\begin{remark}
 	Analogously, $\mathbb{P} \sim \mathbb{Q}$ if and only if $\mathbb{P}(A) = 1 \iff \mathbb{Q}(A) = 1$, or $\mathbb{P}(A) > 0 \iff \mathbb{Q}(A) > 0$ or $\mathbb{P}(A) < 1 \iff \mathbb{Q}(A) < 1$.
\end{remark}

\begin{exbox}
	Take $\Omega = \{1, 2, 3\}$ and $\mathcal{F} = \mathcal{P}(\Omega)$. Let
	\[
	\mathbb{P}(1) = \frac{1}{2}, \qquad \mathbb{P}(2) = \frac{1}{2}, \qquad \mathbb{P}(3) = 0,
	\]
	\[
	\mathbb{Q}(1) = \frac{1}{3}, \qquad \mathbb{Q}(2) = \frac{2}{3}, \qquad \mathbb{Q}(3) = 0.
	\]
	Then $\mathbb{P} \sim \mathbb{Q}$.

	Moreover, let $Z$ be a random variable such that $\mathbb{P}(Z > 0) = 1$, and such that $\mathbb{E}[Z] = 1$. Define $\mathbb{Q}(A) = \mathbb{E}[Z \mathbbm{1}_{A}]$.

	Then $\mathbb{Q}$ is a probability measure on $(\Omega, \mathcal{F})$, and $\mathbb{P} \sim \mathbb{Q}$.

	Indeed, if  $\mathbb{P}(A) = 0$, then $\mathbb{E}[\mathbbm{1}_{A}] = 0$, and so $\mathbb{Q}(A) = \mathbb{E}[Z \mathbbm{1}_{A}] = 0$ by MCT.

	Conversely, if $\mathbb{Q}(A) = 0$, then $\mathbb{E}[Z \mathbbm{1}_{A}] = 0$, so $\mathbb{P}(Z \mathbbm{1}_{A} = 0) = 1$. But since $\mathbb{P}(Z = 0) = 0$, we must have $\mathbb{P}(\mathbbm{1}_{A} = 0) = 0$, hence $\mathbb{P}(A) = 0$.
\end{exbox}

In fact, this is the only such example.

\begin{theorem}[Radon-Nikodym]
	$\mathbb{P} \sim \mathbb{Q}$ if and only if there exists random variable $Z$ with respect to $\mathbb{P}$ such that $\mathbb{P}(Z > 0) = 1$, $\mathbb{E}[Z] = 1$ and
	\[
	\mathbb{Q}(A) = \mathbb{E}[Z \mathbbm{1}_{A}],
	\]
	for all $A \in \mathcal{F}$.
\end{theorem}
The proof of this is in part III advanced probability.

For such $\mathbb{P}, \mathbb{Q}$, we define
\[
\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} = Z.
\]
This is the \emph{Radon-Nikodym derivative}\index{Radon-Nikodym derivative}.

In the first example, $Z(1) = 2/3$, $Z(2) = 4/3$ and $Z(3) = e^2 - \pi$ gives $\mathbb{Q}(A) = \mathbb{P}(Z \mathbbm{1}_{A})$.

\begin{remark}
	If $\mathbb{P} \sim \mathbb{Q}$ and $\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} = Z$, then $\mathbb{E}_{\mathbb{Q}}[X] = \mathbb{E}_{\mathbb{P}}[Z X]$, for any $\mathbb{Q}$-integrable $X$.
\end{remark}

\begin{exbox}
	Let $X$ be defined on $(\Omega, \mathcal{F}, \mathbb{P})$ and $X \sim \Exp(\lambda)$ under $\mathbb{P}$. Then let
	\[
	Z = \frac{u}{\lambda} e^{(\lambda - u)x}, \qquad \mathbb{E}[Z] = \int_0^{\infty} \frac{u}{\lambda} e^{(\lambda - u)x} \lambda e^{-\lambda x} \diff x = \int_0^{\infty} u e^{-u x} \diff x = 1.
	\]
	So if we define $\mathbb{Q}$ by density $Z$, then we want to find the distribution of $X$ under $\mathbb{Q}$. Note that
	\begin{align*}
		\mathbb{E}_{\mathbb{Q}}[f(X)] &= \mathbb{E}_{\mathbb{P}}[Z f(X)] = \int_0^{\infty} \frac{u}{\lambda} e^{(\lambda - u)x} f(x) \lambda e^{-\lambda x} \diff x \\
					      &= \int_0^{\infty} u e^{-u x} f(x) \diff x,
	\end{align*}
	so $X \sim \Exp(u)$ under $\mathbb{Q}$.
\end{exbox}

\begin{definition}
	In a financial model, any measure $\mathbb{Q}$ equivalent to $\mathbb{P}$ such that
	\[
	S_0 = \frac{\mathbb{E}^{\mathbb{Q}}[S_1]}{1 + r},
	\]
	is called \emph{risk-neutral}\index{risk-neutral measure}.
\end{definition}

%lecture 8

\begin{remark}
	\begin{itemize}
		\item[]
		\item The definition of a risk-neutral measure does not depend on the agent's utility.
		\item It does depend on $\mathbb{P}$.
		\item Agents with equivalent beliefs (i.e. agree on the probability 0 events) agree on the risk-neutrality of a measure.
	\end{itemize}
\end{remark}

\begin{theorem}[Marginal Utility Pricing $Z$]
	Let $X^{\ast} \in \mathcal{X}$ maximize $\mathbb{E}[U(X)]$ over $X \in \mathcal{X}$, where $U$ is strictly increasing, differentiable and suitably nice. Let $\mathbb{Q}$ be defined by
	\[
	\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} = \frac{U'(X^{\ast})}{\mathbb{E}[U'(X^{\ast})]}.
	\]
	Then $\mathbb{Q}$ is a risk-neutral measure.
\end{theorem}

We can also say that
\[
\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} \propto U'(X^{\ast}).
\]

\begin{proofbox}
	Note that
	\[
	\frac{\mathbb{E}_{\mathbb{Q}}[S_1]}{1+r} = \frac{\mathbb{E}_{\mathbb{P}}[U'(X^{\ast})S_1]}{(1+r) \mathbb{E}_{\mathbb{P}}[U'(X^{\ast})]} = S_0,
	\]
	by the first marginal utility pricing theorem.
\end{proofbox}

\subsection{Arbitrage}
\label{sub:arbitrage}

We define the concept of arbitrage, which will be used in the fundamental theorem of asset pricing: there exists a risk-neutral measure if and only if there does not exist an arbitrage.

\begin{definition}
	An \emph{arbitrage}\index{arbitrage} $\phi \in \mathbb{R}^{d}$ is such that $\mathbb{P}(\phi^{T}(S_1 - (1+r)S_0) \geq 0) = 1$, and $\mathbb{P}(\phi^{T}(S_1 - (1+r)S_0) > 0) > 0$.
\end{definition}

\begin{remark}
	\begin{itemize}
		\item[]
		\item The definition of arbitrage does not depend on $U$.
		\item It depends on $\mathbb{P}$, but only through the events of measure $0$.
	\end{itemize}
\end{remark}

Given $X \in \mathcal{X}$, and an arbitrage $\phi$, let
\[
X' = X + \phi^{T}(S_1 - (1+r)S_0) \in \mathcal{X}.
\]
Then $U(X') \geq U(X)$ almost surely, and $\mathbb{P}(U(X') > U(X)) > 0$. Hence $\mathbb{E}[U(X')] > \mathbb{E}[U(X)]$, so $X'$ is preferred to $X$.

Hence there cannot be a maximiser of $\mathbb{E}[U(X)]$. Indeed, if $\phi$ is an arbitrage, then so is $n \phi$ for all $n \geq 0$.

Hence, if there exists a utility maximiser, then there exists a risk-neutral measure, and if there exists a utility maximiser, then there is no arbitrage.

\begin{theorem}[Fundamental Theorem of Asset Pricing]
	There exists a risk-neutral measure if and only if there does not exist an arbitrage.
\end{theorem}

\begin{proofbox}
	Suppose there exists a risk-neutral measure. To show there is no arbitrage, let $\phi \in \mathbb{R}^{d}$ be such that $\phi^{T}(S_1 - (1+r)S_0) \geq 0$ $\mathbb{P}$-almost surely. Then by equivalence, $\phi^{T}(S_1 - (1+r)S_0) \geq 0$ $\mathbb{Q}$-almost surely. But
	\[
	\mathbb{E}_{\mathbb{Q}}[\phi^{T}(S_1 - (1+r)S_0)] = \phi^{T}\mathbb{E}_{\mathbb{Q}}[S_1 - (1+r)S_0] = 0,
	\]
	by risk-neutrality. Therefore $\phi^{T}(S_1 - (1+r)S_0) = 0$ $\mathbb{Q}$-almost surely, and hence $\mathbb{P}$-almost surely. So $\phi$ is not an arbitrage.

	Now we show the backwards direction. Assume there is no arbitrage; then we will find a solution to the utility maximisation problem.

	Without loss of generality, assume $\mathbb{E}_{\mathbb{P}}[e^{\theta \cdot S_1}] < \infty$ for all $\theta \in \mathbb{R}^d$. Otherwise, replace $\mathbb{P}$ with $\mathbb{P}'$ where
	\[
	\frac{\diff \mathbb{P}'}{\diff \mathbb{P}} \propto e^{- \|S_1\|^2}.
	\]
	Let
	\[
		f(\theta) = \mathbb{E}[e^{-\theta^T Z}] = - \mathbb{E}[U(X)],
	\]
	where $U(x) = -e^{-x}$, $Z = S_1 - (1+r)S_0$ and $X = \theta^{T} Z$. Let $\theta_k$ be a minimizing sequence, so $f(\theta_k) \to \inf f(\theta)$.
	\begin{enumerate}
		\item If $(\theta_k)$ is bounded, by Bolzano-Weierstrass, there exists a convergent subsequence. So assume $\theta_k \to \theta^{\ast}$. As $f$ is continuous, $f(\theta_k) \to f(\theta^{\ast})$, so $\theta^{\ast}$ is a minimizer. Then
			\[
			\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} \propto e^{-(\theta^{\ast})^{T} (S_1 - (1+r)S_0)}
			\]
			is risk-neutral.

			%lecture 9
		\item Now assume all minimising sequences are unbounded. Recall we assumed there was no arbitrage, meaning if $\phi^{T} Z \geq 0$ almost surely, then $\phi^{T} Z = 0$ almost surely.

			Now I claim we may assume $Z^1, \ldots, Z^d$ are linearly independent, meaning if $\phi^{T}Z = 0$ almost surely, then $\phi = 0$. If not, then we can restrict to a linearly independent submarket, then there is still no arbitrage in the submarket.

			Let $(\theta_k)$ be a minimising sequence with $\|\theta_k\| \to \infty$. Then let $\phi_k = \theta_k/\|\theta_k\|$, so by Bolzano-Weierstrass, we assume $\phi_k \to \phi_0$, where $\|\phi_0\| = 1$. Now we will show that $\phi_0^{T}Z \geq 0$ almost surely. Assuming this, no-arbitrage would imply that $\phi_0^{T}Z = 0$ almost surely, but by linear independence we have $\phi_0 = 0$, which contradicts $\|\phi_0\| = 1$.

			We will now prove this claim. Since $0$ is not a minimiser, $f(0) = 1 \geq f(\theta_k)$ for $k$ large enough. But
			\[
				f(\theta_k) = \mathbb{E}[e^{-\theta_k^{T}Z}] \geq \mathbb{E}[e^{-(\phi_k^{T}Z)\|\theta_k\|} \mathbbm{1}_{\{\phi_0^{T}Z < -\eps, \|Z\| < r\}}]
			\]
			for arbitrary $\eps > 0$, $r > 0$. Now note
			\[
			\phi_k^{T} Z = (\phi_k - \phi_0)^{T} Z + \phi_0^{T} Z \leq \|\phi_k - \phi_0\|r - \eps \leq - \frac{\eps}{2},
			\]
			by choosing $k$ large enough. Hence
			\[
			1 \geq f(\theta_k) \geq e^{\eps \|\theta_k\|/2} \mathbb{P}(\phi_0^{T} Z < -\eps, \|Z\| < r),
			\]
			for all $k \geq N$. Hence
			\[
			\mathbb{P}(\phi_0^{T} Z < -\eps, \|Z\| < r) \leq e^{-\eps/2 \|\theta_k\|} \to 0
			\]
			as $k \to \infty$. Hence taking $r \to \infty$ and $\eps \to 0$, $\mathbb{P}(\phi_0^{T} Z < 0) = 0$.
	\end{enumerate}
\end{proofbox}

\subsection{No-arbitrage Pricing of Contingent Claims}
\label{sub:no_arb_price}

\begin{theorem}
	Given a market with prices $S_0, S_1$ and interest $r$, assume there is no arbitrage. Introduce a claim with payout $Y$ and initial price $\pi$.

	There is no arbitrage in the augmented market if and only if there exists $\mathbb{Q}$ risk-neutral for the original market such that
	\[
	\pi = \frac{1}{1+r} \mathbb{E}_{\mathbb{Q}}[Y].
	\]
	Then the set of all possible no-arbitrage prices $\pi$ is an interval.
\end{theorem}

\begin{proofbox}
	The first part is the fundamental theorem of asset pricing.

	For the last part, we show the possible set of prices is convex. Let
	\[
	\pi_i = \frac{\mathbb{E}_{\mathbb{Q}_i}[Y]}{1 + r},
	\]
	for $i = 0, 1$ and risk-neutral measures $\mathbb{Q}_i$. Let $0 < p < 1$, then let $\mathbb{Q}_p$ have density
	\[
	p \frac{\diff \mathbb{Q}_1}{\diff \mathbb{P}} + (1-p) \frac{\diff \mathbb{Q}_0}{\diff \mathbb{P}} = \frac{\diff \mathbb{Q}_p}{\diff \mathbb{P}}.
	\]
	Note that
	\[
	\frac{\mathbb{E}_{\mathbb{Q}_p}[S_1]}{1 + r} = p \frac{\mathbb{E}_{\mathbb{Q}_1}[S_1]}{1 + r} + (1-p) \frac{\mathbb{E}_{\mathbb{Q}_0}[S_1]}{1+r} = S_0,
	\]
	so $\mathbb{Q}_p$ is risk-neutral. Then
	\[
		p \pi_1 + (1-p) \pi_0 = \frac{\mathbb{E}_{\mathbb{Q}_p}[Y]}{1 + r}
	\]
	is a no-arbitrage price of the claim.
\end{proofbox}

\begin{remark}
	The marginal utility price is a no-arbitrage price, since $U'(X_1^{\ast})$ is proportional to the density of a risk-neutral measure.
\end{remark}

\subsection{Attainable Claims}
\label{sub:att_claim}

\begin{definition}
	A claim is \emph{attainable}\index{attainable claim} if there exists $a \in \mathbb{R}$ and $b \in \mathbb{R}^d$ such that
	\[
	Y = a + b^{T} S_1 = (1+r)x  + b^{T}(S_1 - (1+r)S_0),
	\]
	where
	\[
	x = \frac{a}{1 + r} + b^{T}S_0.
	\]
\end{definition}

For an attainable claim, the indifference price is $x$, for all $U$ and $X_0$. Hence the marginal utility price is the same for all $U$ and $X_0$.

\begin{theorem}[Uniqueness of No-Arbitrage Price]
	If $Y$ is an attainable claim, then there is a unique no-arbitrage price.

	Conversely, if $Y$ has a unique no-arbitrage price, then it is attainable.
\end{theorem}

\begin{proofbox}
	For the first part, suppose $Y = a + b^{T}S_1$. Then
	\[
	\frac{1}{1 + r} \mathbb{E}_{\mathbb{Q}}[Y] = \frac{a}{1 + r} + b^{T}S_0
	\]
	for all risk-neutral measures $\mathbb{Q}$.

	The second part is in the example sheets.
\end{proofbox}

%lecture 10

We look at some examples of attainable claims.

\begin{exbox}
	Let's look at the forward contract, which is the obligation to buy the stock at time $1$ for a fixed price $K$.

	Then the payout is $Y = S_1 - K$. Therefore the unique no arbitrage price is
	\[
	S_0 - \frac{k}{1 + r}.
	\]
	In reality, we should account for dividends, which are incorporated in the time $0$ price, but not the time $1$ price.

	Moreover, in real markets, $K$ is set so that the time 0 price is $\pi = 0$, i.e. $K = (1+r)S_0$.
\end{exbox}

\begin{exbox}
	Let's look at the one-period binomial model, with $-1 < a < b$, $0 < p < 1$ and interest rate $r$ given. Then
	\[
	S_1 =
	\begin{cases}
		S_0(1+b) &\text{with probability } p, \\
		S_0(1+a) &\text{with probability } 1-p.
	\end{cases}
	\]
	Consider a claim with payout $Y = g(S_1)$. This is attainable, since if we let
	\[
	X_1 = (1+r) X_0 + \theta(S_1 - (1+r)S_0) = g(S_1),
	\]
	then we must have
	\begin{align*}
		(1 + r)X_0 + \theta S_0(b - r) = g(S_0(1+b)),\\
		(1+r)X_0 + \theta S_0(a - r) = g(S_0(1+a)).
	\end{align*}
	Hence we can find
	\[
	\theta = \frac{g(S_0(1+b)) - g(S_0(1+a))}{S_0(b-a)},
	\]
	and also $X_0$. There is only no arbitrage in the model if $a < r < b$. Let $\mathbb{Q}$ be a risk-neutral probability with
	\[
	\mathbb{Q}(S_1 = (1+b)S_0) = q = 1 - \mathbb{Q}(S_1 = (1+a)S_0).
	\]
	Then we have
	\[
	q S_0(1+b) + (1-q)S_0 (1+a) = \mathbb{E}_{\mathbb{Q}}[S_1] = (1+r)S_0,
	\]
	hence we get
	\[
	q = \frac{r - a}{b - a}.
	\]
	Note that $a < r < b \iff 0 < q < 1$. Now to find $X_0$, note that
	\[
	\mathbb{E}_{\mathbb{Q}}[g(S_1)] = \mathbb{E}_{\mathbb{Q}}[(1+r)S_0 + \theta(S_1 - (1+r)S_0)] = (1+r)X_0,
	\]
	hence the no-arbitrage price is
	\[
	\pi = \frac{1}{1 + r} \mathbb{E}_{\mathbb{Q}}[g(S_1)] = \frac{q g(S_0(1+b)) + (1-q) g(S_0(1+a))}{1 + r}.
	\]
\end{exbox}

\newpage

\section{Multi-Period Models}
\label{sec:mult_models}

The challenge for multi-period models is to keep track of the information that is revealed at each time period. To do this, we use a filtered probability space: we will let $\mathcal{F}_0, \mathcal{F}_1, \ldots = (\mathcal{F}_n)$ be the $\sigma$-algebras of information available at time $n$. We assume that $\mathcal{F}_n \subseteq \mathcal{F}_{n+1}$.

To informally motivate why we consider $\sigma$-algebras, we let $\mathcal{G}$ be some set of information. Then we can define $\mathbb{P}(A \mid \mathcal{G})$, which is the conditional probability of $A$ happening given $\mathcal{G}$.

We say that $A$ is $\mathcal{G}$-measurable if $\mathbb{P}(A \mid \mathcal{G}) \in \{0, 1\}$.

\begin{exbox}
	Let $\Omega = \{HH, HT, TH, TT\}$ be the outcome of two coin flips, and $\mathcal{G}$ be the information available after the first flip.

	Then $A = \{HH, HT\}$ is $\mathcal{G}$-measurable, because we know whether it happened or not after the first flip, whereas $B = \{HH\}$ is not $\mathcal{G}$-measurable. If the first flip is heads, we still have to know about the second flip.
\end{exbox}

We can then define our $\sigma$-algebra to be the collection of measurable events (we assume it is a $\sigma$-algebra).

\subsection{Measurability}
\label{sub:meas}

Consider a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and $\mathcal{G} \subseteq \mathcal{F}$ a sub-$\sigma$-algebra.

\begin{definition}
	A $\in \mathcal{F}$ is $\mathcal{G}$-measurable $\iff A \in \mathcal{G}$.

	A random variable $X$ is $\mathcal{G}$-measurable if and only if $\{X \leq x\}$ is $\mathcal{G}$-measurable for all $x \in \mathbb{R}$.
\end{definition}

\begin{remark}
	If $X$ is $\mathcal{G}$-measurable, then given $\mathcal{G}$ we know the value of $X$.
\end{remark}

\begin{exbox}
	Let $\Omega$ be our collection of two coin flips, and $\mathcal{G} = \{\emptyset, \Omega, \{HH, HT\}, \{TT, TH\}\}$. Define
	\[
	X(\omega) =
	\begin{cases}
		1 & \omega \in \{HH, HT\}, \\
		0 &\text{else}.
	\end{cases}
	\]
	Then $X$ is $\mathcal{G}$-measurable. However,
	\[
	Y(\omega) =
	\begin{cases}
		2 & \omega = HH,\\
		1 & \omega \in \{HT, TH\},\\
		0 & \omega = TT,
	\end{cases}
	\]
	then $Y$ is not $\mathcal{G}$-measurable.
\end{exbox}

\begin{definition}
	The $\sigma$-algebra generated by $X$, denoted $\sigma(X)$, is the smallest $\sigma$-algebra containing $\{X \leq x\}$ for all $x \in \mathbb{R}$.
\end{definition}

\begin{theorem}
	$Y$ is $\sigma(X)$-measurable if and only if $Y = f(X)$ for a measurable function $f : \mathbb{R} \to \mathbb{R}$.
\end{theorem}

%lecture 11

We will try to model the time $t$ price vector $S_t$. First we need an increasing collection of information $(\mathcal{F}_t)$, and we want $S_t$ to be $\mathcal{F}_t$-measurable.

Recall, for $X$ taking values in $\{x_1, x_2, \ldots\}$, $X$ is $\mathcal{G}$-measurable $\iff \{X = x_i\} \in \mathcal{G}$. Moreover, $X$ is $\mathcal{G}$-measurable $\iff $ there exists $X_n$ $\mathcal{G}$-measurable discrete random variables, such that $X_n \to X$.

Recall $\sigma(X)$, the $\sigma$-algebra generated by $X$. The smallest such $\sigma$-algebra is $\{\emptyset, \Omega\}$.

In discrete time, we will try to extend the fundamental theorem of asset pricing in discrete time: no arbitrage exists if and only if there exists $\mathbb{Q} \sim \mathbb{P}$ such that
\[
\frac{\mathbb{E}^{\mathbb{Q}}[S_t|\mathcal{F}_{t-1}]}{1+r} = S_{t-1},
\]
for all $t$. To make sense of this we will need to define conditional expectations.

Recall, that if $X$ is integrable, then for any event $A$ such that $\mathbb{P}(A) > 0$, we can define
\[
\mathbb{E}[X|A] = \frac{\mathbb{E}[X \mathbbm{1}_{A}]}{\mathbb{P}(A)}.
\]
Also recall that if $Y = \{y_1, y_2, \ldots\}$, with $\mathbb{P}(Y = y_i) > 0$, then $\mathbb{E}[X | Y] = f(Y)$, a random variable in terms of $Y$, where $f(y) = \mathbb{E}[X|Y = y]$, for $y \in \{y_1, y_2, \ldots\}$.

\begin{proposition}
	In the above set up, we have the projection property:
	\[
	\mathbb{E}[X \mathbbm{1}_{G}] = \mathbb{E}[f(Y) \mathbbm{1}_{G}],
	\]
	for any $\sigma(Y)$-measurable event $G$.
\end{proposition}

\begin{proofbox}
	Let $G = \{Y \in B\}$, for $B \subseteq \{y_1, y_2, \ldots\}$. Then
	\begin{align*}
		\mathbb{E}[f(Y) \mathbbm{1}_{G}] &= \mathbb{E}\left[ \left( \sum_i \frac{\mathbb{E}[X \mathbbm{1}_{\{Y = y_i\}}]}{\mathbb{P}(Y = y_i)} \mathbbm{1}_{\{Y = y_i\}} \right) \mathbbm{1}_{\{Y \in B\}} \right] \\
						 &= \sum_i \frac{\mathbb{E}[X \mathbbm{1}_{\{Y = y_i\}}]}{\mathbb{P}(Y = y_i)} \mathbb{E}[\mathbbm{1}_{\{Y = y_i\}} \mathbbm{1}_{\{Y \in B\}}] \\
						 &= \sum_{y_i \in B} \mathbb{E}[X \mathbbm{1}_{\{Y = y_i\}}] = \mathbb{E}\left[ X \left( \sum_{y_i \in B} \mathbbm{1}_{\{Y = y_i\}} \right) \right] \\
						 &= \mathbb{E}[X \mathbbm{1}_{G}].
	\end{align*}
\end{proofbox}

This motivates the following.

\begin{definition}
	Let $X$ be integrable, and $\mathcal{G}$ a $\sigma$-algebra. An integrable $\mathcal{G}$-measurable random variable $Z$ is a \emph{conditional expectation}\index{conditional expectation} of $X$ given $\mathcal{G}$ if and only if
	\[
	\mathbb{E}[X \mathbbm{1}_{G}] = \mathbb{E}[Z \mathbbm{1}_{G}],
	\]
	for all $G \in \mathcal{G}$.
\end{definition}

\begin{theorem}
	Conditional expectations exist and are unique.
\end{theorem}

\begin{proofbox}
	Existence follows from analysis (the fact $L^2$ is complete). We prove uniqueness.

	Suppose $Z_0, Z_1$ are two conditional expectations of $X$ given $\mathcal{G}$:
	\[
	\mathbb{E}[X \mathbbm{1}_{G}] = \mathbb{E}[Z_0 \mathbbm{1}_{G}] = \mathbb{E}[Z_1 \mathbbm{1}_{G}],
	\]
	for all $G \in \mathcal{G}$. So subtracting, $\mathbb{E}[(Z_0 - Z_1)\mathbbm{1}_{G}] = 0$. But now we can take anything to be $G$.

	Take first $G = \{Z_0 \geq Z_1\}$, Then $(Z_0 - Z_1) \mathbbm{1}_{G} \geq 0$, but since the expectation is 0, we have $\mathbb{P}(Z_0 \leq Z_1) = 1$. Similarly $\mathbb{P}(Z_1 \leq Z_0) = 1$, so $Z_0 = Z_1$ a.e.
\end{proofbox}

The expectation of $X$ given $\mathcal{G}$ is $\mathbb{E}[X\mid\mathcal{G}]$. We can also define conditional expectations in terms of random variables, as
\[
\mathbb{E}[X \mid Y] = \mathbb{E}[X \mid \sigma(Y)],
\]
which agrees with our example from before.

Moreover, if $X$ is square-integrable, and $Y$ is square-integrable and $\mathcal{G}$-measurable, then the projection property gives
\[
\mathbb{E}[XY] = \mathbb{E}[\mathbb{E}[X \mid \mathcal{G}] Y].
\]
Therefore, $\mathbb{E}[X \mid \mathcal{G}]$ minimizes $\mathbb{E}[(X - Z)^2]$ over all $\mathcal{G}$-measurable random variables $Z$. We can think of this conditional expectation, thus, as the ``best guess'' for $X$ given our information $\mathcal{G}$.

\begin{proofbox}
	Let $Z$ be $\mathcal{G}$-measurable, and $Y = -Z + \mathbb{E}[X \mid \mathcal{G}]$. Then
	\begin{align*}
		\mathbb{E}[(X - Z)^2] &= \mathbb{E}[(X - \mathbb{E}[X\mid\mathcal{G}] + Y)^2] \\
				      &= \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{G}])^2] + \mathbb{E}[Y^2] + 2 \underbrace{\mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{G}]) Y]}_{0} \\
				      &\geq \mathbb{E}[(X - \mathbb{E}[X \mid \mathcal{G}])^2].
	\end{align*}
\end{proofbox}

%lecture 12

Properties of conditional expectation are:
\begin{enumerate}
	\item Linearity. Given constant $a, b$ and integrable random variables $X, Y$, then
		\[
		\mathbb{E}[aX + bY \mid \mathcal{G}] = a \mathbb{E}[X\mid\mathcal{G}] + b \mathbb{E}[Y\mid\mathcal{G}].
		\]
	\item Non-negative. If $X \geq 0$ almost surely, then $\mathbb{E}[X \mid \mathcal{G}] \geq 0$.
	\item Pulling out what is known. If $Y$ is $\mathcal{G}$-measurable, then
		\[
		\mathbb{E}[XY \mid \mathcal{G}] = Y \mathbb{E}[X \mid \mathcal{G}].
		\]
	\item Independence. If $X$ is independent of $\mathcal{G}$, i.e. $\sigma(X)$ independent of $\mathcal{G}$, then
		\[
		\mathbb{E}[X \mid \mathcal{G}] = \mathbb{E}[X].
		\]
	\item Conditional Jensen. If $f$ is convex, then
		\[
		\mathbb{E}[f(X) \mid \mathcal{G}] \geq f(\mathbb{E}[X \mid \mathcal{G}]).
		\]
	\item Holding known random variable fixed and averaging over an independent one. Suppose $X$ is independent of $\mathcal{G}$, and $Y$ is $\mathcal{G}$-measurable. Then,
		\[
		\mathbb{E}[f(X, Y) \mid \mathcal{G}] = \mathbb{E}[f(X, y)] \biggr|_{y = Y}.
		\]
\end{enumerate}

\begin{exbox}
	Say $X, Y$ are independent $N(0, 1)$, and $\mathcal{G} = \sigma(Y)$. Let $\phi$ be the pdf of a $N(0, 1)$ random variable. Then,
	\begin{align*}
		\mathbb{E}[f(X, Y) \mid \mathcal{G}] &= \int f(x, y) \phi(x) \diff x \biggr|_{y = Y} = \int f(x, Y) \phi(x) \diff x.
	\end{align*}
\end{exbox}

The proof of these is using the definitions. For Jensen's, we redo the proof of unconditional Jensen using the supporting line theorem.

\newpage

\section{Martingales}
\label{sec:mart}

\begin{definition}
	A family $(\mathcal{F}_t)$ of $\sigma$-algebras, such that $\mathcal{F}_s \subseteq \mathcal{F}_t$ for all $s \leq t$ is called a \emph{filtration}\index{filtration}.
\end{definition}

In this course, we use the convention that $\mathcal{F}_0 = \{\emptyset, \Omega\}$. If $X$ is $\mathcal{F}_0$-measurable, then $X$ is constant. Also $\mathbb{E}[Y \mid \mathcal{F}_0] = \mathbb{E}[Y]$ for any $Y$.

\begin{definition}
	A \emph{stochastic process}\index{stochastic process} is a collection of random variables $(X_t)$. It is \emph{adapted}\index{adapted stochastic process} to a filtration $(\mathcal{F}_t)$ if and only if $X_t$ if $\mathcal{F}_t$-measurable for all $t \geq 0$.
\end{definition}

\begin{definition}
	The filtration generated by a process $(X_t)$ is $\mathcal{F}_t = \sigma(X_s, 0 \leq s \leq t)$.
\end{definition}

This is the smallest filtration such that the process is adapted.

\begin{definition}
	A \emph{martingale}\index{martingale} $(X_t)$ with respect to a filtration $(\mathcal{F}_t)$ is an adapted, integrable process such that
	\[
	\mathbb{E}[X_t \mid \mathcal{F}_s] = X_s
	\]
	almost surely, for all $0 \leq s \leq t$.
\end{definition}

It is equivalent to check that $\mathbb{E}[X_t - X_s \mid \mathcal{F}_s] = 0$, since $X_s$ is $\mathcal{F}_s$ measurable hence the conditional expectation is itself.

\begin{proposition}[Tower Property]\index{tower property}
	If $\mathcal{G} \subseteq \mathcal{H}$, then
	\[
	\mathbb{E}[\mathbb{E}[Y\mid\mathcal{G}]\mid\mathcal{H}] = \mathbb{E}[\mathbb{E}[Y \mid \mathcal{H}] \mid \mathcal{G}] = \mathbb{E}[Y \mid \mathcal{G}].
	\]
\end{proposition}

\begin{proposition}
	In discrete time, $(X_t)$ is a martingale with respect to $(\mathcal{F}_t)$ if and only if
	\[
	\mathbb{E}[X_t \mid \mathcal{F}_{t-1}] = X_{t-1},
	\]
	for all $t \geq 1$.
\end{proposition}

\begin{proofbox}
	If we know $(X_t)$ is a martingale, then the above holds, using the definition with $s = t - 1$.

	Conversely, if the above holds for all $t \geq 1$, then note that
	\begin{align*}
		\mathbb{E}[X_{s+k}\mid\mathcal{F}_s] &= \mathbb{E}[\mathbb{E}[X_{s + k} \mid \mathcal{F}_{s + k - 1}] | \mathcal{F}_s] = \mathbb{E}[X_{s + k - 1} \mid \mathcal{F_s}],
	\end{align*}
	so the martingale property holds by induction.
\end{proofbox}

Our motivation for this is as follows: the fundamental theorem of asset pricing says that there is no arbitrage if and only if there exists $\mathbb{Q} \sim \mathbb{P}$ such that
\[
\mathbb{E}^{\mathbb{Q}}\left[ \frac{S_t}{1+r} \mid \mathcal{F}_{t-1} \right] = S_{t-1},
\]
which, rephrasing, says that
\[
\left( \frac{S_t}{(1 + r)^t} \right)
\]
is a $\mathbb{Q}$-martingale.

\begin{exbox}[Doob Martingale]
	Let $Y$ be an integrable random variable, and given a filtration $(\mathcal{F}_t)$, let $X_t = \mathbb{E}[Y \mid \mathcal{F}_t]$. Then $(X_t)$ is a martingale.

	This follows from the tower law:
	\[
		\mathbb{E}[X_t \mid \mathcal{F}_s] = \mathbb{E}[\mathbb{E}[Y \mid \mathcal{F}_t] \mid \mathcal{F}_s] = \mathbb{E}[Y \mid \mathcal{F}_s] = X_s.
	\]
\end{exbox}

%lecture 13

\subsection{Discrete Time Martingales}
\label{sub:dtm}

We look at another example of martingales. Let $X_1, X_2, \ldots$ be independent with $\mathbb{E}[X_n] = 0$ for all $n$, and integrable.

Let $\mathcal{F}_0 = \{\emptyset, \Omega\}$ and $\mathcal{F}_n = \sigma(X_1, \ldots, X_n)$. Then define $S_0 = 0$ and $S_n = X_1 + \cdots + X_n$.

We claim that $(S_n)$ is a martingale. These are adapted as $S_n$ is a function of $X_1, \ldots, X_n$, hence $\mathcal{F}_n$-measurable, and integrable as it is a sum of integrable random variables.

Moreover the martingale property is true, since
\[
\mathbb{E}[S_n - S_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[X_n \mid \mathcal{F}_{n-1}] = \mathbb{E}[X_n] = 0,
\]
as $X_n$ is independent of all other random variables.

\begin{definition}
	A (discrete-time) process $(H_n)$ is \emph{previsible}\index{previsible} or \emph{predictable}\index{predictable} if $H_n$ is $\mathcal{F}_{n-1}$-measurable.
\end{definition}

\begin{definition}
	The \emph{martingale transform}\index{martingale transform} of a previsible process $(H_n)$ with respect to an adapted process $(X_n)$ is
	\[
	Y_0 = 0, \qquad Y_n = \sum_{k = 1}^n H_k(X_k - X_{k-1}).
	\]
	This is also known as a discrete time stochastic integral.
\end{definition}

This martingale transform is adapted, as $Y_n$ is a function of $\mathcal{F}_n$-measurable random variable.

\begin{theorem}
	The martingale transform of a bounded previsible process with respect to a martingale is a martingale.
\end{theorem}

\begin{proofbox}
	We already checked adaptedness. $Y_n$ is integrable since
	\[
	|Y_n| \leq \sum_{k = 1}^n |H_k| |X_k - X_{k-1}| \leq C \sum_{k = 1}^n |X_k - X_{k-1}|,
	\]
	where $|H_k| \leq C$. Moreover $|X_k|$ is integrable by definition, since it is a martingale. Finally,
	\[
	\mathbb{E}[Y_n - Y_{n-1} \mid \mathcal{F}_{n-1}] = \mathbb{E}[H_n(X_n - X_{n-1}) \mid \mathcal{F}_{n-1}] = H_n \mathbb{E}[X_n - X_{n-1} \mid \mathcal{F}_{n-1}] = 0,
	\]
	since $(X_n)$ is a martingale.
\end{proofbox}

Lets see a couple of applications. The first is from finance. Beginning with the interest rate $r$, with $d$ assets and where asset $i$ has price $S^i_n$ at time $n$.

Say that between time $n-1$ and time $n$ the investor holds $\theta^0_n$ in the bank, and $\theta^i_n$ shares of asset $i$. Let $X_n$ be the wealth of the investor at time $n$. Then,
\begin{align*}
	X_{n-1} &= \theta^0_n + \theta_n^T S_{n-1}, \\
	X_n &= \theta^0_n(1+r) + \theta_n^T S_n.
\end{align*}
This is known as the self-financing condition. Eliminating $\theta^0_n$, we get
\[
X_n = (1+r)X_{n-1} + \theta_n^T(S_n - (1+r)S_{n-1}),
\]
and hence
\[
\frac{X_n}{(1 + r)^n} = \frac{X_{n-1}}{(1 + r)^{n-1}} + \theta^T_n \left( \frac{S_n}{(1 + r)^n} - \frac{S_{n-1}}{(1 + r)^{n-1}} \right),
\]
therefore we find that
\[
\frac{X_n}{(1 + r)^n} = X_0 + \sum_{k = 1}^n \theta_k^T \left( \frac{S_n}{(1 + r)^n} - \frac{S_{n-1}}{(1 + r)^{n-1}} \right).
\]
So the discounted wealth of the investor is the initial wealth, plus the martingale transform of the portfolio process with respect to discounted asset prices (here $X_n/(1 + r)^n$ is the discounted wealth, and $S_n/(1 + r)^n$ is the discounted asset prices).

We will assume that $\theta_n$ is $\mathcal{F}_{n-1}$-measurable to avoid clairvoyance, i.e. $(\theta_n)$ is previsible.

\subsection{Stopping Times}
\label{sub:stop}

\begin{definition}
	A \emph{stopping time}\index{stopping time} for a filtration $(\mathcal{F}_t)$ is a random variable $T$ defined in $\mathbb{N} \cup \{\infty\}$ for discrete time, or $[0, +\infty]$ for continuous time, such that $\{T \leq t\}$ is $\mathcal{F}_t$-measurable for all $t \geq 0$.
\end{definition}

\begin{exbox}
	In discrete time, let $(X_n)$ be adapted. Let $T = \inf\{n \geq  0 \mid X_n > 0\}$, the first time $X_n$ is positive, with the convention that $\inf \emptyset = \infty$. Then $T$ is a stopping time. Indeed,
	\[
		\{T \leq n\} = \bigcup_{k = 0}^n \{ X_k > 0 \} \in \mathcal{F}_n
	\]
	for all $n$.

	A non-example is as follows: let $T = \sup\{n \geq 0 \mid X_n > 0\}$, i.e. the last time that $X_n$ is positive, where $\sup \emptyset = 0$. Then
	\[
		\{T \leq n \} = \bigcap_{k \geq n+1} \{X_k \leq 0\},
	\]
	is not necessarily measurable.
\end{exbox}

%lecture 14

\subsection{Optimal Stopping}
\label{sub:opt_stop}

\begin{definition}
	Given an adapted process $(X_t)$ and a stopping time $T$, the \emph{stopped process}\index{stopped process} is $(X_{t \wedge T})$, i.e.
	\[
	X_{t \wedge T} =
	\begin{cases}
		X_t & \text{for } t \leq T,\\
		X_T & \text{for } t \geq T.
	\end{cases}
	\]
\end{definition}

From now on, time is discrete.

\begin{proposition}
	The stopped process $(X_{n \wedge T})$ is $X_0$, plus the martingale transform of $(1_{\{n \leq T\}})$ with respect to $(X_n)$.
\end{proposition}

\begin{proofbox}
	Indeed, we have
	\[
		X_0 + \sum_{k = 1}^n \mathbbm{1}_{\{k \leq T\}} (X_k - X_{k-1}) =
		\begin{cases}
			X_n & \text{if } T > n, \\
		X_T &\text{if } T \leq n.
		\end{cases}
	\]
	We also have to check that the integrand is previsible: indeed, $\{n \leq T\} = \{T \leq n-1\}^{c} \in \mathcal{F}_{n-1}$, so $(\mathbbm{1}_{\{n \leq T\}})$ is previsible.
\end{proofbox}

\begin{theorem}
	Stopped martingales are martingales.
\end{theorem}

\begin{proofbox}
	Note $(\mathbbm{1}_{\{n \leq T\}})$ is bounded, and martingale transforms of a bounded previsible process is a martingale.
\end{proofbox}

\begin{theorem}[Optimal Stopping Theorem]
	Let $(X_n)$ be a martingale and $T$ a stopping time, such that $T < \infty$ almost surely. Suppose that $(X_{n \wedge T})$ is bounded. Then $\mathbb{E}[X_T] = X_0$.
\end{theorem}

Note there is nothing to prove if $T$ is not random; this follows directly from the definition of a martingale.

\begin{proofbox}
	Note that $\mathbb{E}[X_{n \wedge T}] = X_0$, since $(X_{n \wedge T})$ is a martingale.

	By assumption, $|X_{n \wedge T}| < C$ almost surely for all $n$, where $C > 0$ is a constant. Note $X_{n \wedge T} \to X_T$ almost surely as $n \to \infty$, since $T < \infty$ almost surely, so $|X_T| < C$ almost surely. Then
	\begin{align*}
		|\mathbb{E}[X_T] - X_0| &= |\mathbb{E}[X_T - X_{T \wedge n}]| \leq \mathbb{E}[|X_T - X_{T \wedge n}]| \\
					&\leq 2C \mathbb{P}(T > n) \to 0,
	\end{align*}
	as $n \to \infty$.
\end{proofbox}

\begin{remark}
	The condition $(X_{n \wedge T})$ is bounded can be replaced by $(X_{n \wedge T})$ is uniformly integrable. Another useful criterion is $|X_{n \wedge T}| \leq Z$ almost surely, for all $n$, where $\mathbb{E}[Z] < \infty$.
\end{remark}

\begin{exbox}
	Let $(X_n)$ be a simple, symmetric random walk on $\mathbb{Z}$, starting at $X_0 = 0$, meaning $X_n = \xi_1 + \xi_2 + \cdots + \xi_n$, where $(\xi_n)$ is iid, with $\mathbb{P}(\xi_n = 1) = \mathbb{P}(\xi_n = -1) = 1/2$.

	Fix $a, b > 0$ integers, and let $T = \inf\{n \geq 0 \mid X_n \in \{-a, b\}\}$. This is a stopping time, since it is the first hitting time of an adapted process. Moreover $|X_{n \wedge T}| \leq \max\{a, b\}$ almost surely, and by Markov chains $T < \infty$ almost surely.

	So $\mathbb{E}[X_T] = X_0$ by optimal stopping theorem, hence $\mathbb{P}(X_T = -a)(-a) + \mathbb{P}(X_T = b) b = 0$, allowing us to solve
	\[
	\mathbb{P}(X_T = b) = \frac{a}{a + b} = 1 - \mathbb{P}(X_T = -a).
	\]

	For a non-example, in the same setting let $\tau = \inf\{n \geq 0 \mid X_n = -a\}$. Again this is a stopping time, and $\tau < \infty$ almost surely, but
	\[
	\mathbb{E}[X_\tau] = -a \neq 0 = X_0.
	\]
	This is as $(X_{n \wedge \tau})$ is not bounded.
\end{exbox}

\begin{exbox}
	We now try to calculate $\mathbb{E}[z^{\tau}]$, where $0 < z < 1$, and $\tau$ is the same stopping time as before. From example sheets, we know $(z^n w^{X_n})$ is a martingale, for some $w$. Indeed,
	\[
	\mathbb{E}[z^n w^{X_n} \mid \mathcal{F}_{n-1}] = z^n w^{X_{n-1}} \mathbb{E}[w^{\xi_n}] = z \left( \frac{w + 1/w}{2} \right) z^{n-1} w^{X_{n-1}},
	\]
	so $w$ solves $w + 1/w = 2/z$. Hence by optimal stopping theorem,
	\[
	\mathbb{E}[z^\tau w^{X_\tau}] = z^0 w^0 = 1 \implies \mathbb{E}[z^\tau] = w^a.
	\]
	However there are two possible values of $w$. We want $z^{n \wedge \tau} w^{X_{n \wedge \tau}}$ to be bounded. To do this we use the value of $w$ that is less than $1$.
\end{exbox}

%lecture 15

\subsection{Super and Sub-Martingales}
\label{sub:ss_mart}

\begin{definition}
	An adapted integrable process $(X_t)$ is a \emph{supermartingale}\index{supermartingale} if and only if $X_s \geq \mathbb{E}[X_t \mid \mathcal{F}_s]$ almost surely, and a \emph{submartingale}\index{submartingale} if and only if $X_s \leq \mathbb{E}[X_t \mid \mathcal{F}_s]$ almost surely, for all $0 \leq s \leq t$.
\end{definition}

\begin{remark}
	The supermartingale condition is equivalent to $\mathbb{E}[X_t - X_s \mid \mathcal{F}_s] \leq 0$ almost surely, for all $0 \leq s \leq t$.

	In discrete time, this is equivalent to $\mathbb{E}[X_{n+1} \mid \mathcal{F}_n] \leq X_n$ almost surely, for all $n \geq 0$.
\end{remark}

\begin{theorem}
	The martingale transform of a non-negative bounded previsible process with respect to a supermartingale is a supermartingale.
\end{theorem}

\begin{proofbox}
	We sketch the proof. Let
	\[
	Y_k = \sum_{k = 1}^n H_k (X_k - X_{k-1}),
	\]
	where $X$ is a supermartingale, and $H$ is bounded, non-negative and previsible. Then boundedness is easy, and
	\begin{align*}
		\mathbb{E}[Y_n - Y_{n-1} \mid \mathcal{F}_n] &= \mathbb{E}[H_n(X_n - X_{n-1}) \mid \mathcal{F}_{n-1}] = H_n \mathbb{E}[X_n - X_{n-1} \mid \mathcal{F}_{n-1}] \leq 0.
	\end{align*}
\end{proofbox}

\begin{theorem}
	Let $(X_n)$ be a supermartingale. Let $S$ and $T$ be stopping times such that $0 \leq S \leq T$ almost surely. Then $(X_{n \wedge T} - X_{n \wedge S})$ is a supermartingale.
\end{theorem}

\begin{proofbox}
	Note that
	\[
		X_{n \wedge T} - X_{n \wedge S}= \sum_{k = 1}^n \mathbbm{1}_{\{S < k \leq T\}} (X_k - X_{k-1}).
	\]
	The indicator functions are bounded, non-negative and $\mathcal{F}_{k-1}$-measurable, so this is a martingale transform, and then the last theorem applies.
\end{proofbox}

\begin{theorem}[Optional Sampling Theorem]
	Let $(X_n)$ be a supermartingale, and $S, T$ be bounded stopping times such that $0 \leq S \leq T$. Then
	\[
	\mathbb{E}[X_T] \leq \mathbb{E}[X_S].
	\]
\end{theorem}

\begin{remark}
	If $X$ is a martingale, then $\mathbb{E}[X_T] = \mathbb{E}[X_S] = X_0$ for bounded $S, T$.

	Moreover if $S, T$ are not bounded, but $(X_{n \wedge T})$ is bounded, then $\mathbb{E}[X_T] \leq \mathbb{E}[X_S]$ by the argument from the optional stopping theorem.
\end{remark}

\begin{proofbox}
	Suppose $0 \leq S \leq T \leq N$ almost surely. Let $Y_n = X_{n \wedge T} - X_{n \wedge S}$. Then $Y$ is a supermartingale, from before, but notice that $Y_N = X_T - X_S$.

	Then we claim that $\mathbb{E}[Y_n] \leq Y_0 = 0$. But this follows from the definition of a supermartingale, with $s = 0$ and $t = N$.
\end{proofbox}

\newpage

\section{Controlled Markov Processes}
\label{sec:cont}

\begin{definition}
	A process $(X_t)$ is \emph{Markov}\index{Markov process} for a filtration if $(X_t)$ is adapted, and
	\[
	\mathbb{P}(X_t \in A \mid \mathcal{F}_s) = \mathbb{P}(X_t \in A \mid X_s)
	\]
	almost surely, for all $0 \leq s \leq t$ and measurable subsets $A$.
\end{definition}

In discrete time, it suffices that
\[
\mathbb{P}(X_n \in A \mid \mathcal{F}_{n-1}) = \mathbb{P}(X_n \in A \mid X_{n-1}).
\]
In discrete time, we have the following equivalent view: there exists a sequence of independent $(\xi_n)$ and a function $G$ such that $X_n = G(n, X_{n-1}, \xi_n)$ for all $n$.

\begin{definition}
	A \emph{controlled Markov process}\index{controlled Markov process} $(X_n^U)$, given a previsible $(U_n)$ is such that
	\[
	X_n^U = G(n, X_{n-1}^U, U_n, \xi_n),
	\]
	for some function $G$ and independent sequence $(\xi_n)$.

	Here $U$ is the control variable.
\end{definition}

\begin{exbox}
	Given $X_0$ and a time horizon $N$, we might want to find $U$ to maximize
	\[
	\mathbb{E}\left[g(X^U_N) + \sum_{k = 1}^N f(k, U_k)\right].
	\]
	The \emph{Bellman equation}\index{Bellman equation} associated to this problem is:
	\begin{align*}
		V(N, x) &= g(x), \\
		V(n-1, x) &= \max_u \mathbb E \left[ V(n, G(n, x, u, \xi_n)) + f(n, u) \right],
	\end{align*}
	for all $1 \leq n \leq N$.
\end{exbox}

\begin{definition}
	The \emph{value function}\index{value function} for the problem is the function
	\[
	V(n, x) = \sup_{(U_k)} \mathbb{E}\left[ g(X_N^U) + \sum_{k = n + 1}^N f(k, U_k) \mid X_n^U = x \right].
	\]
\end{definition}

Under certain conditions, the solution to the Bellman equation is the value function.

%lecture 16

\subsection{Dynamic Programming Principle}
\label{sub:dpp}

\begin{theorem}[Dynamic Programming Principle]
	Let $V$ solve the associated Bellman equations:
	\begin{align*}
		V(N, x) &= g(x), \\
		V(n-1, x) &= \max_u \mathbb E \left[ V(n, G(n, x, u, \xi_n)) + f(n, u) \right], \tag{$\ast$}
	\end{align*}
	for all $x$ and for all $1 \leq n \leq N$. Let $u^\ast(n, x)$ be the maximizer of $(\ast)$.

	Let $X_0^\ast = X_0$, $U_n^\ast = u^\ast(n, X_{n-1}^\ast)$ and $X_n^\ast = G(n, X_{n-1}^\ast, U_n^\ast, \xi_n)$, so $X_n^{U^\ast} = X_n^\ast$.

	Then $U^\ast$ is the optimal control, and $V$ is the value function for the problem.
\end{theorem}

\begin{proofbox}
	We assume that everything is integrable and measurable. To do this, we use the `martingale principle of optimal control': we find a process which is a martingale for the optimal control, and a supermartingale in general.

	In our case, let
	\[
	M_n^U = \sum_{k = 1}^n f(n, U_k) + V(n, X_n^U).
	\]
	Then we claim $M^U$ is a supermartingale, and $M^{U^\ast}$ is a martingale. This is as
	\begin{align*}
		\mathbb{E}[M_n &- M_{n-1} \mid \mathcal{F}_{n-1}]  \\
		&= \mathbb{E}[f(n, U_n) + V(n, X_n^U) - V(n-1, X_{n-1}^U) \mid \mathcal{F}_{n-1}] \\
		&= \mathbb{E}[f(n, U_n) + V(n, G(n, X_{n-1}^U, U_n, \xi_n)) \mid \mathcal{F}_{n-1}] - V(n-1, X_{n-1}^U) \\
		&= \mathbb{E}[f(n, u) + V(n, G(n, x, u, \xi_n))] \biggr|_{\substack{u = U_n \\ x = X_{n-1}^U}} - V(n-1, X_{n-1}^U) \\
		&\leq f(n, u^\ast(n, x)) + \mathbb{E}[V(n, G(n, x, u^\ast(n, x), \xi_n))] \biggr|_{x = X_{n-1}^U} \!\!\!\!- V(n-1, X_{n-1}^U) \\
		&\leq 0,
	\end{align*}
	with equality if and only if $U = U^\ast$. So
	\begin{align*}
		&\mathbb{E}\left[ \sum_{k = n+1}^N f(k, U_k) + g(X_N^U) \mid X_n^U = x \right] \\
		&= \mathbb{E}\left[ M_N^U - \sum_{k = 1}^n f(k, u_k) \mid X_{n}^U = x \right] \\
		&= \mathbb{E}\left[\mathbb{E}\left[ M_N^U - \sum_{k = 1}^n f(k, u_k) \mid \mathcal{F}_{n} \right] \mid X_{n}^u = x \right] \\
		&\leq \mathbb{E}\left[ M_n^U - \sum_{k = 1}^n f(k, U_k) \mid X_n^U = x \right] \\
		&= \mathbb{E}[V(n, X_n^U) \mid X_n^U = x] = V(n, x),
	\end{align*}
	with equality if and only if $U = U^\ast$.
\end{proofbox}

\begin{exbox}
	We look at an optimal investment problem. Take $d = 1$ risky asset, and suppose $S_n = S_{n-1} Z_n$, where $(Z_n)$ are independent, with interest rate $r$.

	Between times $n-1$ and $n$, the investor consumes $c_n$ between $0$ and $X_{n-1}$, and holds $\theta_n$ shares, so the wealth equation is
	\[
	X_n = (1 + r)(X_{n-1} - c_n) + \theta_n(S_n - (1 + r)S_{n-1}) = G(n, X_{n-1}, U_n, \xi_n),
	\]
	where $U_n = (c_n, S_{n-1} \theta_n)$ and
	\[
	G(n, X, (c, \eta), \xi) = (1 + r)(x - c) + \eta(\xi - (1 + r)),
	\]
	where $\eta = S_{n-1} \theta_n$. Our goal is to maximize
	\[
	\mathbb{E}\left[ \sum_{k = 1}^n u(c_k) + u(X_N) \right]
	\]
	over $(c_n, \theta_n)$ given $X_0$, where $u$ is a given utility function. The Bellman equation is then
	\begin{align*}
		V(N, x) &= u(x), \\
		V(n-1, x) &= \max_{\substack{0 \leq c \leq x \\ \eta}} \left[ u(c) + \mathbb{E}\left[ V(n, (1+r)(x - c) + \eta(\xi_n - (1+r))) \right] \right],
	\end{align*}
	for all $x$ and for all $1 \leq n \leq N$. This is generally intractible, but our idea is to let $u$ be a specific utility function, e.g. the CRRA utility.
%lecture 17

	In this case, we have
	\[
	u(x) = \frac{x^{1-R}}{1 - R},
	\]
	for some value $R > 0$, $R \neq 1$. We guess that $V(n, x) = A_n u(x)$, for all $0 \leq n \leq N$, and where $A_n$ is to be determined. This works for $n = N$ with $A_N = 1$. Suppose it works for $n = k$. Then the right hand side is
	\begin{align*}
		&\max_{c, \eta} \left[ u(c) + A_k \mathbb{E}\left[ u((1 + r)(x - c) + \eta(\xi - (1+r)))\right] \right] \\
		&= u(x) (1 - R) \max_{c, \eta} \biggl[ u \left( \frac{c}{x} \right) + u \left(1 - \frac{c}{x} \right) A_k (1 - R)  \\
		& \qquad \times \mathbb{E}\left[ u\left((1 + r) + \frac{\eta}{x - c} (c - (1 + r)) \right)  \right] \biggr].
	\end{align*}
	Let $\alpha = (1 -R) \max \mathbb{E} U(1 + r + t (\xi - (1 + r)))$, and $t^\ast$ be the argmax. Letting $t = \frac{\eta}{x - c}$, and $s = \frac{c}{x}$, then this becomes
	\[
		u(x)(1 - R) \max_{ 0 \leq s \leq 1} [u(s) + u(1 - s) A_k \alpha ].
	\]
	At the maximum, the derivative will be 0. Hence
	\[
	s^{-R} = (1-s)^{-R}(1 - R)A_K \alpha \implies S^\ast_k = \frac{1}{1 + (\alpha A_k)^{1/k}}.
	\]
	Putting this in again, we get $u(x)(1 + (A_k \alpha)^{1/R})^{R}$. We want this to equal $u(x) A_{k-1}$. Hence solving
	\[
	A_{k-1} = (1 + (A_k \alpha)^{1/R})^{R}, \qquad A_N = 1,
	\]
	this gives us
	\[
	A_n = \left( \frac{1 - \alpha^{(N - n + 1)/R}}{1 - \alpha^{1/R}} \right)^R,
	\]
	for all $0 \leq n \leq N$. Plugging this back into $S_n^\ast$,
	\[
	c_n^\ast = X_{n-1}^{\ast} S_n^{\ast} = \frac{X_{n-1}^{\ast}}{1 + (A_n\alpha)^{1/R}} = \frac{X_{n-1}^\alpha(1 - \alpha^{1/R})}{1 - \alpha^{(N+n)/R}},
	\]
	which gives us
	\[
	\theta_n^{\ast} = \frac{\eta_n^{\ast}}{S_{n-1}} = \frac{t^{\ast} (X_{n-1}^\ast - c_n^\ast)}{S_{n-1}},
	\]
	which is calculable. Therefore, we can calculate the optimal wealth:
	\begin{align*}
		X_n^\ast &= (1 + r)(X_{n-1}^\ast - c_n^\ast) + \eta_n^\ast (\xi_n - (1 + r)) \\
			 &= X_{n-1}^\ast \frac{\alpha^{1/R}(1 - \alpha^{(N - n + 1)/R})}{(1 - \alpha^{(N - n + 1)/R})}[(1 + r) + t^\ast(\xi_n - (1 + r))],
	\end{align*}
	or something like that.
\end{exbox}

\subsection{Infinite Horizon Problem}
\label{sub:inf_hor}

We set $X_n = G(X_{n-1}, U_n, \xi_n)$, where $(\xi_n)$ are IID. Notice that there is no $n$-dependence in this.

Our goal is now to maximize
\[
\mathbb{E}\left[ \sum_{k=  1}^\infty \beta^{k-1} f(U_k) \right],
\]
where $0 < \beta < 1$ is the \emph{subjective discount factor}. The Bellman equation now relates to
\[
	V(x) = \max \{f(u) + \beta \mathbb{E} V(G(x, u, \xi)) \}.
\]

\begin{theorem}
	Suppose $f(u) \geq 0$ for all $u$, and $u^\ast (x) = \mathrm{argmax}(f(u) + \beta \mathbb{E} V(x, u, \xi))$, where $V$ is the solution to the Bellman equation.

	Then given $X_0$, let $U_n^\ast = u^\ast(X_{n-1}^\ast)$, $X_0^\ast = X_0$, and $X_n^\ast = G(X_{n-1}^\ast, U_n^\ast, \xi_n)$.

	Suppose that $\beta^n \mathbb{E} V(X_n^\ast) \to 0$. Then $U^\ast$ is optimal, and $V$ is the value function.
\end{theorem}

To prove this we need the monotone convergence theorem: if $(Z_n)$ is a sequence of random variables such that $0 \leq Z_n \leq Z_{n+1}$ almost surely, then let $Z_\infty = \sup Z_n = \lim Z_n$. Then
\[
\mathbb{E}[Z_\infty] = \lim \mathbb{E}[Z_n].
\]

\begin{proofbox}
	We prove that $V$ is the value function. Pick a control $U$, and let
	\[
	M_n^U = \sum_{k = 1}^n \beta^{k-1} f(U_k) + \beta^n V(X_n^U).
	\]
	We claim that this is a supermartingale for all $U$, and a martingale when $U = U^\ast$. Indeed,
	\begin{align*}
		\mathbb{E}[M_n - M_{n-1} \mid \mathcal{F}_{n-1}] &= \beta^{n-1} [f(U) + \beta \mathbb{E}[V(X_n^U) \mid \mathcal{F}_{n-1}] - V(X_{n-1}^U)] \\
								 &\leq 0,
	\end{align*}
	by the Bellman equation, with equality if $U = U^\ast$ by the definition of $U^\ast$. Hence
	\begin{align*}
		V(X) &= M_0^U \geq \mathbb{E}[M_N^U] = \mathbb{E}\left[ \sum_{k=1}^n \beta^{k-1} f(U_k)\right] + \beta^{n} \mathbb{E}[V(X_N^U)],
	\end{align*}
	with equality if $U = U^\ast$. Since $V(x) \geq 0$ for all $x$, and $f(u) \geq 0$, we can use MCT:
	\[
	V(X) \geq \mathbb{E} \sum_{k = 1}^N \beta^{k-1} f(U_k) \to \mathbb{E}\left[ \sum_{k = 1}^\infty \beta^{k-1} f(U_k) \right],
	\]
	for $U = U^\ast$. Hence
	\begin{align*}
		V(X) &= \mathbb{E} \sum_{k = 1}^N \beta^{k-1} f(U_k^\ast) + \beta^{n} \mathbb{E} V(X_N^{U^\ast}) \to \mathbb{E} \sum_{k = 1}^\infty \beta^{k-1} f(U_k^\ast).
	\end{align*}
\end{proofbox}

%lecture 18

\subsection{Optimal Stopping}%
\label{sub:opt_stop}

Our goal is the following: given a Markov process $(X_n)$, and a horizon $N > 0$, we wish to find a stopping time $T^\ast \leq N$ to maximize $\mathbb{E}[g(X_T)]$.

Suppose that $X_n = G(n, X_{n-1}, \xi_n)$, for $(\xi_n)$ independent. Then
\[
X_{n \wedge T} =
\begin{cases}
	G(n, X_{(n-1) \wedge T}, \xi_N) & \text{if } T \geq n, \\
	X_{(n-1) \wedge T} & \text{if } T \leq n-1.
\end{cases}
\]
So we have $X_{n \wedge T} = H(n, X_{(n-1) \wedge T}, U_n, \xi_n)$, where $U_n = \mathbbm{1}_{\{T \leq n-1\}}$. Then,
\[
H(n, x, u, \xi) =
\begin{cases}
	G(n, x, \xi) & u = 0, \\
	x & u = 1.
\end{cases}
\]
\begin{theorem}
	Let $V(N, x) = g(x)$, and $V(n-1, x) = \max \{ g(x), \mathbb{E}[V(n, G(n, x, \xi_n))]\}$, for $1 \leq n \leq N$. Then
	\[
		V(n, x) = \max_{\substack{n \leq T \leq N \\ \text{stopping times}}} \mathbb{E}[g(X_T) \mid X_n = x].
	\]
\end{theorem}

\newpage

\section{Multiperiod Models}%
\label{sec:mult_mod}


\subsection{Multiperiod Arbitrage}%
\label{sub:mult_arb}


Recall that the time $n$ wealth is
\[
X_n = (1+r) X_{n-1} + \theta_n^T (S_n - (1+r) S_{n-1}).
\]
No consumption is equivalent to self-financing. Then
\[
\frac{X_n}{(1+r)^n} = X_0 + \sum_{k = 1}^n \theta_k^T \left( \frac{S_k}{(1+r)^k} - \frac{S_{k-1}}{(1+r)^{k-1}}\right).
\]

\begin{definition}
	An $N$-period \emph{arbitrage}\index{arbitrage} is a previsible process $(\phi_n)$ such that
	\[
	\sum_{n = 1}^n \phi_n^T \left( \frac{S_n}{(1+r)^n} - \frac{S_{n-1}}{(1+r)^{n-1}} \right) \geq 0
	\]
	almost surely, with strict inequality with positive probability.
\end{definition}

\begin{definition}
	A \emph{risk-neutral measure}\index{risk-neutral measure} $\mathbb{Q}$ is equivalent to $\mathbb{P}$, and such that $(S_n/(1+r)^n)$ is a $\mathbb{Q}$-martingale, i.e.
	\[
	\frac{\mathbb{E}[S_n \mid \mathcal{F}_{n-1}]}{1+r} = S_{n-1},
	\]
	for all $n \geq 1$.
\end{definition}

\begin{remark}
	The risk-neutral measures are sometimes called equivalent martingales measures.
\end{remark}

\begin{theorem}[Fundamental Theorem of Asset Pricing]
	There is no $N$-period arbitrage if and only if there exists a risk-neutral measure for $(S_n)$.
\end{theorem}

The proof is by essentially reusing the one-period argument $N$ times.

\subsection{Binomial Model}%
\label{sub:bin_mod}

Let $d = 1$, and $S_n = S_{n-1} \xi_n$, where $S_0 > 0$ and $\xi_n > 0$ for all $n$, with
\[
0 < \mathbb{P}(\xi_n = 1 + b \mid \mathcal{F}_{n-1}) = 1 - \mathbb{P}(\xi_n = 1 + a \mid \mathcal{F}_{n-1}) < 1
\]
almost surely, for all $n \geq 1$, where $a, b$ are given constants. Hence the probability of following any path is positive.

\begin{theorem}
	There exists a risk-neutral measure $\mathbb{Q}$ if and only if $a < r < b$, in which case $(\xi_n)$ are iid under $\mathbb{Q}$, with
	\[
	\mathbb{Q}(\xi_n = 1 + b) = \frac{r - a}{b - a},
	\]
	for all $n$. In particular, $\mathbb{Q}$ is unique.
\end{theorem}

\begin{proofbox}
	Suppose $\mathbb{Q}$ exists. Then
	\begin{align*}
	\frac{\mathbb{E}^{\mathbb{Q}}[S_n \mid \mathcal{F}_{n-1}]}{1 + r} &= S_{n-1} = \frac{\mathbb{E}^{\mathbb{Q}}[S_{n-1}\xi_n \mid \mathcal{F}_{n-1}]}{1+r} \\
										  &= \frac{S_{n-1}}{1 + r} \Bigl( \mathbb{Q}(\xi_n = 1 + b \mid \mathcal{F}_{n-1})(1 + b) \\
										  &\qquad+ \mathbb{Q}(\xi_n = 1 + a \mid \mathcal{F}_{n-1})(1 + a)\Bigr).
	\end{align*}
	Solving this, we get that
	\[
	\mathbb{Q}(\xi_n = 1 + b \mid \mathcal{F}_{n-1}) = \frac{r - a}{b - a}, \qquad \mathbb{Q}(\xi_n = 1 + a \mid \mathcal{F}_{n-1}) = \frac{b - r}{b - a}.
	\]
	So $\mathbb{Q} \sim \mathbb{P}$ exists if and only if $a < r < b$, since both probabilities must be positive.

	Since the conditional distribution of $\xi_n$ given $\mathcal{F}_{n-1}$ is not random, $\xi_n$ is independent of $\mathcal{F}_{n-1}$, and it is the same for all $n$. So $(\xi_n)$ are iid under $\mathbb{Q}$.
\end{proofbox}

\begin{remark}
	$(S_n)$ is a $\mathbb{Q}$-Markov process. Moreover, all that we need to show that $(\xi_n)$ is adapted to $(\mathcal{F}_n)$, but we usually take $\mathcal{F}_n = \sigma(\xi_1, \ldots, \xi_n)$.
\end{remark}

%lecture 19

\newpage

\section{Pricing and Hedging Contingent Claims}%
\label{sec:pricing_claims}

\subsection{European Claims}%
\label{sub:european}

\begin{definition}
	A \emph{European}\index{European claim} contingent claim is specified by a non-random maturity date $N$, and $\mathcal{F}_N$-measurable payout $Y$.
\end{definition}

\begin{exbox}
	A European \emph{call}\index{European call} with maturity $N$ and strike $K$ has payout
	\[
	Y = (S_N - K)^{+}.
	\]
	Similarly, the European \emph{put}\index{European put} has payout
	\[
	Y = (K - S_N)^+.
	\]
	There are other path-dependent claims: for example the look-back, with payout
	\[
	Y = S_N - \min_{0 \leq n \leq N} S_n.
	\]
\end{exbox}

Let's think about pricing these. To do this, consider the binomial model with $d = 1$, and $S_n = S_{n-1} \xi_n$, where $\xi_n$ takes values in $\{1+a,1+b\}$, for $a < r < b$.

There is a unique risk-neutral measure, so there is a unique no-arbitrage price of the European claim:
\[
\pi_n = \frac{1}{(1 + r)^{N - n}} \mathbb{E}^{\mathbb{Q}}[Y \mid \mathcal{F}_n] = f_n(S_0, \ldots, S_n),
\]
assuming that $\mathcal{F}_n = \sigma(S_0, \ldots, S_n)$, and by measurability, for some $f_n : \mathbb{R}^{n+1} \to \mathbb{R}$.

\begin{theorem}
	Let
	\[
	\theta_n = \frac{f_n(S_0, \ldots, S_{n-1}, S_{n-1}(1+b)) - f_n(S_0, \ldots, S_{n-1}, S_{n-1}(1+a))}{S_{n-1}(b-a)}.
	\]
	The wealth process starting at $X_0 = \pi_0$, employing trading strategy $(\theta_n)$ is such that $X_n = \pi_n$, for $n \leq N-1$, and $X_N = Y$ almost surely.
\end{theorem}

\begin{proofbox}
	For each $n$, there exists a unique $\mathcal{F}_{n-1}$-measurable solution $(x_{n-1}, b_n)$ to
	\[
		(1+r)x_{n-1} + b_n(S_n - (1 + r)S_{n-1}) = \pi_n,
	\]
	since this is equivalent to:
	\begin{align*}
		(1+r)x_{n-1} + b_n S_{n-1}(b - r) &= f_n(S_0, \ldots, S_{n-1}, S_{n-1}(1+b)),\\
		(1+r)x_{n-1} + b_n S_{n-1}(a - r) &= f_n(S_0, \ldots,S_{n-1}, S_{n-1}(1+a)).
	\end{align*}
	The solution is $b_n = \theta_n$, and
	\begin{align*}
		(1 + r)_{n-1} &= \mathbb{E}^{\mathbb{Q}}[\pi_n \mid \mathcal{F}_{n-1}] = \mathbb{E}^{\mathbb{Q}}[\mathbb{E}^{\mathbb{Q}}[Y \mid \mathcal{F}_n] \mid \mathcal{F}_{n-1}] \\
			      &= (1+r)\pi_{n-1},
	\end{align*}
	by the tower property. So $x_{n-1} = \pi_{n-1}$.

	So starting at $X_0 = \pi_0$, and using strategy $(\theta_n)$, this equation says that $X_n = \pi_n$ for all $n \leq N$, and $X_N = Y$, by induction.
\end{proofbox}

\begin{remark}
	If $Y = g(S_N)$ for some function $g$, the claim is sometimes called a \emph{vanilla claim}\index{vanilla claim}. Otherwise it is called \emph{exotic}\index{exotic claim}.

	For vanilla claims, we have
	\[
	\pi_n = \frac{\mathbb{E}^{\mathbb{Q}}[g(S_N) \mid \mathcal{F}_n]}{(1+r)^{N - n}} = \frac{\mathbb{E}^{\mathbb{Q}}[g(S_N)\mid S_n]}{(1 + r)^{N - n}} = V(n, S_n)
	\]
	for some function $V$, by the Markov property of $(S_N)$ under $\mathbb{Q}$. In fact, we can say that
	\[
		V(n, s) \frac{1}{(1 + r)^{N - n}} \sum_{k = 0}^{N - n} \binom{N-n}{k} q^{k}(1 - q)^{N - n - k} g(s(1+b)^k(1+a)^{N - n - k}),
	\]
	where $q = \mathbb{Q}(\xi_1 = 1 + b) = (r-a)/(b-a)$. This satisfies
	\[
		(1 + r)V(n-1, s) = q V(n, s(1+b)) + (1 - q) V(n, s(1 + a)),
	\]
	which is an analogue to the Black-Scholes PDE.
\end{remark}

\subsection{American Claim}%
\label{sub:american}

\begin{definition}
	These are specified by an adapted process $(Y_n)_{0 \leq n \leq N}$, where $Y_n$ is the payout if the claim is exercised at time $n$.

	The time to exercise is chosen by the holder of the claim.
\end{definition}

Consider a binomial model, where $\mathbb{Q}$ is the risk-neutral measure. Let $(Y_n)$ be the payout of the American claim; for example $Y_n = (S_n - K)^{+}$ for the American call. Let
\begin{align*}
	\pi_n &= Y_N, \\
	\pi_{n-1} &= \max\left\{Y_{n-1}, \frac{\mathbb{E}^{\mathbb{Q}}[\pi_n \mid \mathcal{F}_{n-1}]}{1 + r} \right\}.
\end{align*}
Note $\pi_n \geq Y_n$ for all $n$. Moreover, the price process $(\pi_n)$ is a supermartingale in general, since
\[
\pi_n \geq \frac{\mathbb{E}[\pi_{n+1} \mid \mathcal{F}_n]}{1 + r},
\]
but moreover we know that $(\pi_n/(1+r)^n)$ is a martingale up to time
\[
T^\ast = \inf\{ n \mid \pi_n = Y_n\}.
\]
To hedge an American claim, we hedge as if it is a European claim up to time $T^\ast$, i.e.
\[
\theta_n = \frac{f_n(S_0, \ldots, S_{n-1}, S_{n-1}(1+b)) - f_n(S_0, \ldots, S_{n-1}, S_{n-1}(1+a))}{S_{n-1}(b - a)},
\]
where $f_n(S_0, \ldots, S_n) = \pi_n$.

% lecture 20

\newpage

\section{Continuous Time Finance}%
\label{sec:continuous}

For motivation take a binomial model with $d = 1$, and
\[
S_n = S_0 \xi_1 \cdots \xi_n,
\]
so that
\[
\log S_k = \log S_0 + X_1 + \cdots + X_n,
\]
where $X_n = \log \xi_n$. Assume that $(X_n)$ are IID. To change to the continuous time limit, let $t = n \delta$, where $\delta$ is a small parameter, so
\[
\hat S_t = S_n = S_{t/\delta},
\]
and hence
\[
\log \hat S_t = \log S_0 + \mu t + \sigma W_t,
\]
where
\[
\mu = \frac{\mathbb{E}[X]}{\delta}, \qquad \sigma = \frac{1}{\delta} \Var(X), \qquad W_t = \frac{X_1 + \cdots + X_{t/\delta} - t \mu}{\sigma}.
\]
$W$ has the following properties:
\begin{itemize}
	\item $W_t - W_s$ is independent of $(W_u)_{u \leq s}$.
	\item $\mathbb{E}[W_t - W_s] = 0$, and $\Var(W_t - W_s) = t - s$.
	\item $W_t - W_s \sim N(0, t - s)$ as $\delta \to 0$, for fixed $s, t$, i.e. by taking $n = t/\delta \to \infty$, and $m = s/\delta \to \infty$.
\end{itemize}

\subsection{Brownian Motion}%
\label{sub:brown}

\begin{definition}
	A \emph{Brownian motion}\index{Brownian motion} is a continuous-time process $(W_t)$ such that:
	\begin{itemize}
		\item $W_0 = 0$.
		\item $W_t - W_s$ is independent of $(W_u)_{u \leq s}$, for all $0 \leq s \leq t$.
		\item $W_t - W_s \sim N(0, t-s)$ for all $0 \leq s \leq t$.
		\item $t \mapsto W_t$ is continuous, i.e. for all $\omega \in \Omega$, $t \mapsto W_t(\omega)$ is continuous.
	\end{itemize}
	
\end{definition}

\begin{theorem}[Wiener]
	Brownian motion exists, i.e. there exists $(\Omega, \mathcal{F}, \mathbb{P})$ and measurable functions $W_t : \Omega \to \mathbb{R}$ satisfying these definitions.
\end{theorem}

\begin{proposition}
	Brownian motion is a martingale (on its own filtration).
\end{proposition}

\begin{proofbox}
	$W$ is adapted to its own filtration. It is integrable since normal random variables are integrable, and
	\[
	\mathbb{E}[W_t - W_s \mid \mathcal{F}_s] = \mathbb{E}[W_t - W_s] = 0,
\]
since $W_t - W_s = N(0, t - s)$, which has mean 0.
\end{proofbox}

\begin{proposition}
	Brownian motion is a Markov process.
\end{proposition}

\begin{proofbox}
	Let $g$ be bounded. Then,
	\begin{align*}
		\mathbb{E}[g(W_t) \mid \mathcal{F}_s] &= \mathbb{E}[g(W_s + W_t - W_s) \mid \mathcal{F}_s] \\				      &= \mathbb{E}[g(x + W_t - W_s)] \biggr|_{x = W_s} \\
						      &= \mathbb{E}[g(W_s + W_t - W_s)\mid W_s] \\
						      &= \mathbb{E}[g(W_t) \mid W_s].
	\end{align*}
\end{proofbox}

\begin{definition}
	A \emph{Gaussian process}\index{Gaussian process} $(X_t)$ is such that $(X_{t_1}, \ldots, X_{t_n})$ is jointly normal for all $0 \leq t_1 < t_2 < \cdots < t_n$.
\end{definition}

\begin{proposition}
	Brownian motion is Gaussian.
\end{proposition}

\begin{proofbox}
	Fix $0 \leq t_1 < \cdots < t_n$, and constants $a_1, \ldots, a_n$. Then,
	\[
	\sum_{k = 1}^n a_k W_{t_k} = \sum_{k = 1}^n b_k(W_{t_k} - W_{t_{k-1}}),
	\]
	where $t_0 = 0$, and $b_k = a_1 + \cdots + a_k$. Then the sum of independent normals is normal.
\end{proofbox}

\begin{proposition}
	$(W_t)$ is a Brownian motion if and only if it is a continuous Gaussian process with $\mathbb{E}[W_t] = 0$ for all $t$, and $\mathbb{E}[W_sW_t] = s$ for all $0 \leq s \leq t$.
\end{proposition}

\begin{proofbox}
	Suppose $W$ is a Brownian motion. Then it s Gaussian and continuous, and $\mathbb{E}[W_t] = 0$, since $W_t \sim N(0, t)$. Moreover,
	\begin{align*}
		\mathbb{E}[W_s W_t] &= \mathbb{E}[W_s^2] - \mathbb{E}[W_s(W_t - W_s)] \\
				    &= s + \mathbb{E}[W_s] \mathbb{E}[W_t - W_s] = s.
	\end{align*}
	Now suppose $W$ is continuous, Gaussian, $\mathbb{E}[W_t] = 0$ and $\mathbb{E}[W_s W_t] = s$, for all $0 \leq s \leq t$.
	
	Then $\Var(W_t) = \mathbb{E}[W_t^2] = t$, $\Var(W_t - W_ts = \Var(W_t) + \Var(W_s) - 2 \Cov(W_t, W_s) = t + s - 2s = t - s$, hence $W_t - W_s \sim N(0, t-s)$.

	Now for $u < s \leq t$, $\Cov(W_u, W_t - W_s) = \Cov(W_u, W_t) - \Cov(W_u, W_s) = u - u = 0$. Hence by Gaussianity, $W_t - W_s$ is independent of $W_u$, and as it is jointly Gaussian, it is independent of $(W_u)_{u \leq s}$.
\end{proofbox}

\begin{proposition}
	Let $W$ be a Brownian motion. Then consider:
	\begin{enumerate}[\normalfont(i)]
		\item $\hat W_t = c W_{t/c^2}$, for any constant $c \neq 0$.
		\item $\hat W_t = W_{t+T} - W_T$, for $T \geq 0$ a constant.
		\item $\hat W_t = t W_{1/t}$, $\hat W_0 = 0$.
	\end{enumerate}
	Then these are all Brownian motions.
\end{proposition}

These follow from the previous proposition. The last one is as $\mathbb{E}[\hat W_t] = t \mathbb{E}[W_{1/t}] = 0$, and
\[
\mathbb{E}[\hat W_s \hat W_t] = st \mathbb{E}[W_{1/s} W_{1/t}] = st/t = s.
\]

% lecture 21

\begin{theorem}
	Let $(W_t)$ be a Brownian motion, and $T_a = \inf\{t \geq 0 \mid W_t = a\}$. Then $T_a < \infty$ almost surely.
\end{theorem}

\begin{proofbox}
	Consider the case $a > 0$. We must show that
	\[
	\sup_{t \geq 0} W_t \geq a,
	\]
	almost surely. For any $a, b  0$ and $c > 0$,
	\begin{align*}
		\mathbb{P}(a < \sup_{t \geq 0} W_t < b) &= \mathbb{P}(a < \sup_{t \geq 0} c W_{t/c^2} < b ) \\
							&= \mathbb{P}(a/c < \sup_{s \geq 0} W_{s} \leq b/c) \to 0,
	\end{align*}
	as $c \to  \infty$, and for any $(a, b) \supseteq [0, \infty)$. Hence $Z = \sup_{t \geq 0} W_t \in \{0, \infty\}$ almost surely.

	So it satisfies to show that $\sup W_t \neq 0$ almost surely. Let $\hat Z = \sup_{t \geq 1}(W_t - W_1) \overset{d}{=} Z$. So $\hat Z \in \{0, \infty\}$ almost surely.

	Note that $\{\hat Z = \infty\}$ if and only if $\{Z = \infty\}$, as $\sup_{0 \leq t \leq 1} W_t$ is finite by the continuity of Brownian motion. Hence,
	\begin{align*}
		\mathbb{P}(Z = 0) &= \mathbb{P}(Z = 0, \hat Z = 0) \leq \mathbb{P}(W_1 \leq 0, \hat Z = 0) \\
				  &= \mathbb{P}(W_1 \leq 0) \mathbb{P}(\hat Z = 0) = \frac{1}{2} \mathbb{P}(Z = 0).
	\end{align*}
	Therefore $\mathbb{P}(Z = 0) = 0$, so $\mathbb{P}(Z = \infty) = 1$.
\end{proofbox}

Brownian motion also has the strong Markov property.

\begin{theorem}
	Let $(W_t)$ be a Brownian motion, and $T$ a finite stopping time. The process $(W_{t + T} - W_T)$ is also a Brownian motion, independent of $(W_t)_{0 \leq t \leq T}$.
\end{theorem}

The proof of this is omitted.

\begin{theorem}[Reflection Principle]
	Let $(W_t)$ be a Brownian motion, and $a \in \mathbb{R}$. Let
	\[
	\tilde W_t =
	\begin{cases}
		W_t & \text{if } 0 \leq t < T_a,\\
		2a - W_t & \text{if } t \geq T_a.
	\end{cases}
	\]
	Then $(\tilde W_t)$ is a Brownian motion.
\end{theorem}

\begin{proofbox}
	Note that since $T_a$ is a stopping time, $W_{s + T_a} - W_{T_a} = W_{s + T_a} - a$ is a Brownian motion independent of $(W_t)$ for $t \leq T_a$. But now notice
	\begin{align*}
		2a - W_{s + T_a} &= a + (a - W_{s + T_a}) \overset{d}= a + (W_{s + T_a} - a) = W_{s + T_a},
	\end{align*}
	hence $\tilde W_t \overset d= W_t$.
\end{proofbox}

Hence we get the following:
\[
\mathbb{P}(\max_{0 \leq s \leq t} W_s \geq a, W_t \leq b) = \mathbb{P}(W_t \geq 2a - b),
\]
for $a \geq 0, b \leq a$.

\begin{proofbox}
	Note that, taking the same $\tilde W_t$ as before,
	\[
	\mathbb{P}(\max_{0 \leq s \leq t} W_s \geq a, W_t \leq b) = \mathbb{P}(\tilde W_t \geq 2a - b) = \mathbb{P}(W_t \geq 2a - b).
	\]
\end{proofbox}

Consider $M_t = \max_{0 \leq s \leq t}W_s$. Then the joint distribution of $(M_t, W_t)$ is
\begin{align*}
	F_{M_t, W_t}(a, b) &= \mathbb{P}(M_t < a, W_t < b) = \mathbb{P}(W_t < b) - \mathbb{P}(M_t < a, W_t > b) \\
			   &= \mathbb{P}(W_t \leq b) - \mathbb{P}(W_t \geq 2a - b) = \Phi\left(\frac{b}{\sqrt t} \right) - \left( 1 - \Phi \left( \frac{2a - b}{\sqrt t} \right) \right) \\
			   &= \Phi \left( \frac{b}{\sqrt t} \right) + \Phi \left( \frac{2a - b}{\sqrt t} \right) - 1.
\end{align*}
Here $\Phi(x) = \mathbb{P}(Z \leq x)$, where $Z \sim N(0, 1)$. Now we see that
\begin{align*}
	f_{M_t, W_t}(a, b) &= \frac{\partial^2}{\partial a \partial b}F_{M_t, W_t}(a, b) = \frac{\partial}{\partial b} \left( \frac{\partial}{\partial a} \Phi \left( \frac{2a - b}{ \sqrt t} \right) \right) \\
			   &= \frac{\partial}{\partial b} \left( \frac{2}{\sqrt t} \phi\left( \frac{2a - b}{\sqrt t} \right) \right) = \frac{2}{\sqrt t} \left( - \frac{1}{\sqrt t} \right) \phi' \left( \frac{2a - b}{\sqrt t} \right) \\
			   &= \frac{2(2a - b)}{t^{3/2}} \phi \left( \frac{2a - b}{ \sqrt t} \right).
\end{align*}

\begin{proposition}
	For suitable $g$ and $a \geq 0$,
	\[
	\mathbb{E}[g(W_t) \mathbbm{1}_{(M_t \geq a)}] = \mathbb{E}[(g(W_t) + g(2a - W_t))\mathbbm{1}_{(W_t \geq a)}].
	\]
\end{proposition}

\begin{proofbox}
	Note that
	\begin{align*}
		\mathbb{E}[g(W_t)\mathbbm{1}_{(M_t \geq a)}] &= \mathbb{E}[g(W_t) \mathbbm{1}_{(M_t \geq a, W_t \geq a)}] + \mathbb{E}[g(W_t) \mathbbm{1}_{(M_t \geq a, W_t \leq a)}] \\
							     &= \mathbb{E}[g(W_t)\mathbbm{1}_{(W_t \geq a)}] + \mathbb{E}[g(W_t) \mathbbm{1}_{(\tilde W_t \geq a)}] \\
							     &= \mathbb{E}[g(W_t) \mathbbm{1}_{(W_t \geq a)}] + \mathbb{E}[g(2a - W_t) \mathbbm{1}_{(W_t \geq a)}].
	\end{align*}
\end{proofbox}

Hence, we see that
\begin{align*}
	F_{M_t}(a) &= \mathbb{P}(M_t \leq a) = \mathbb{P}(M_t \leq a, W_t \leq a) = 2 \Phi \left( \frac{a}{\sqrt t} \right) - 1 \\
		   &= F_{|W_t|}(a),
\end{align*}
so $M_t$ has the same distribution as $|W_t|$. Hence, for any $a \geq 0$,
\begin{align*}
	\mathbb{P}(T_a \leq t) &= \mathbb{P}(M_t \geq a) = 1 - F_{M_t}(a) \\
			       &= 2 \left(1 - \Phi \left( \frac{a}{\sqrt t} \right) \right) = 2 \Phi \left(\frac{-a}{\sqrt t} \right).
\end{align*}
Hence $T_a$ has density
\begin{align*}
	f_{T_a}(t) &= \frac{\diff}{\diff t} \left( 2 \Phi \left( - \frac{\alpha}{\sqrt t} \right) \right) = \frac{2}{t^{3/2}} \phi\left( - \frac{\alpha}{\sqrt t} \right) = \frac{a}{t^{3/2}} \phi \left( \frac{a}{\sqrt t} \right).
\end{align*}

% lecture 22

\subsection{Cameron-Martin Theorem}%
\label{sub:cm_thm}

To motivate this section, let $X \sim N(0, 1)$. Then,
\[
\mathbb{E}[e^{aX - a^2/2} f(X)]=  \mathbb{E}[f(X + a)],
\]
for $a \in \mathbb{R}$. This was proven in example sheet 1. Now, let $X \sim N_d(0, I_{d})$, so multivariate. Then,
\[
\mathbb{E}[e^{a^T x - \|a\|^2/2} f(X)] = \mathbb{E}[f(X + a)],
\]
for all $a \in \mathbb{R}^d$. We extend this theorem to Brownian motions.

\begin{theorem}[Cameron-Martin]
	Let $(W_t)$ be a Brownian motion, and $f : C[0, T] \to \mathbb{R}$ be suitable. Then,
	\[
	\mathbb{E}[e^{c W_T - c^2T/2} f((W_t)_{0 \leq t \leq T})] = \mathbb{E}[f((W_t + ct)_{0 \leq t\leq T})].
	\]
\end{theorem}

\begin{proofbox}
	By monotone class theorem, it is enough to consider functions of the form
	\[
	f(\omega) = g(\omega(t_1), \omega(t_2), \ldots, \omega(t_d)),
	\]
	for some function $g : \mathbb{R}^d \to \mathbb{R}$, since pointwise evaluations generate the $\sigma$-algebra on $C[0, T]$. Then,
	\begin{align*}
		\mathbb{E}[e^{cW_t - c^2T/2}&f((W_t)_{0 \leq t \leq T})] = \mathbb{E}[e^{cW_T - c^2T/2}g(W_{t_1}, \ldots, W_{t_d})] \\
									&= \mathbb{E}\left[e^{\sum c\sqrt{t_k - t_{k-1}} Z_k - \sum c^2(t_k - t_{k-1})/2} g\left( \left( \sum \sqrt{t_k - t_{k-1}} Z_k \right)  \right) \right].
	\end{align*}
	Now let $a_n = c \sqrt{t_n - t_{n-1}}$, for $1 \leq n \leq d$, and so this equals
	\begin{align*}
		\mathbb{E}\left[ g \left( \sum_{k = 1}^n \sqrt{t_k - t_{k-1}} (Z_k + c \sqrt{t_k - t_{k-1}}) \right) \right] = \mathbb{E}[g((Wt_n + ct_n)_{1 \leq n \leq d})].
	\end{align*}
	
\end{proofbox}

\begin{exbox}
	Consider $\mathbb{P}(\max_{0 \leq t \leq T} (W_t + ct) \leq a)$, a probability of a Brownian motion with drift. Then,
	\begin{align*}
		\mathbb{P}( \max_{0 \leq t \leq T} (W_t + ct) \leq a) &= \mathbb{E}[\mathbbm{1}_{(\max_{0 \leq t \leq T}(W_t + ct) \leq a}] \\
								      &= \mathbb{E}[e^{c W_t - c^2T/2} \mathbbm{1}_{(\max _{ 0 \leq t \leq T} W_t \leq a}] \\
								      &= \mathbb{E}[e^{cW_t - c^2T/2} \mathbbm{1}_{(W_t \leq a)}] - \mathbb{E}[e^{c(2a - W_t) - c^2T/2} \mathbbm{1}_{(W_t \geq a)}] \\
								      &= \mathbb{P}(W_T + cT \leq a) - e^{2ca}\mathbb{P}(W_T - cT \geq a) \\
								      &= \Phi \left( \frac{a - cT}{\sqrt T} \right) - e^{2ca} \Phi \left( \frac{-a - cT}{\sqrt T} \right).
	\end{align*}
\end{exbox}

Here is a reformulation of Cameron-Martin that is useful in the financial setting:

Let $(W_t)$ be a Brownian motion under $\mathbb{P}$, and let
\[
\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} = e^{c W_tT- c^2T/2}.
\]
Let $\hat W_t = W_t - ct$. Then $(\hat W_t)$ is a Brownian motion under $\mathbb{Q}$.

\begin{proofbox}
	Let $f : C[0, T] \to \mathbb{R}$ be suitable. Then,
	\begin{align*}
		\mathbb{E}^{\mathbb{Q}}[f((\hat W_t)_{0 \leq t \leq T})] &= \mathbb{E}^{\mathbb{P}}[e^{cW_T - c^2T/2} f((W_t - ct)_{0 \leq t \leq T})] \\
									 &= \mathbb{E}^{\mathbb{P}}[f((W_t)_{0 \leq t \leq T})].
	\end{align*}
	Hence the law of $\hat W$ under $\mathbb{Q}$ is the same as of $W$ under $\mathbb{P}$.
\end{proofbox}

\subsection{Brownian Motion and the Heat Equation}%
\label{sub:bm_heat}

Our motivation is as follows. Let $(X_n)$ be a simple, symmetric random walk. Let $u(n, x) = \mathbb{E}[g(X_n + x)]$, for $x \in \mathbb{Z}$. Then,
\[
u(n+1, x) =  \frac{1}{2} u(n, x + 1) + \frac{1}{2}u(n, x-1),
\]
from the Markov property of a random walk.

\begin{proposition}
	Let $(W_t)$ be a Brownian motion. Let
	\[
		u(t, x) = \mathbb{E}[g(W_t + x)] = 'int \frac{g(z + x) e^{-z^2/2t}}{\sqrt{2\pi t}} \diff z.
	\]
	Then,
	\[
	\frac{\partial u}{\partial t} = \frac{1}{2} \frac{\partial^2 u}{\partial x^2}.
	\]
\end{proposition}

\begin{proofbox}
	We can prove this by just differentiating the integral formula. Indeed, this is just
	\[
		\int g(\sqrt t z + x) \frac{e^{-z^2}/2}{\sqrt{2 \pi}} \diff z.
	\]
\end{proofbox}


% lecture 23

\newpage

\section{Black-Scholes Model}%
\label{sec:bs}

We are looking at extending the binomial model to continuous time.

To do this, we need to look at interest rate. In the multi-period model, at time $n$ the interest is $(1 + r)^n$. If $r = \hat r \delta$, for $\delta$ the length of the time period, and $n = t/\delta$, this converges to $e^{\hat r t}$. We now drop the hat, and view $r$ as the interest per time.

Recall that in the binomial model $S_n = S_0 \xi_1 \cdots \xi_n = S_0 e^{\mu t + \sigma W_t} = \hat S_t$. In the Black-Scholes model,\index{Black-Scholes model}  we drop the hats, so
\[
S_t = S_0 e^{\mu t + \sigma W_t},
\]
where $W$ is a Brownian motion, and $\mu, \sigma$ are constants with $\sigma > 0$.

\begin{definition}
	A \emph{risk-neutral measure}\index{risk-neutral measure}  $\mathbb{Q}$ is equivalent to $\mathbb{P}$, and such that $(e^{rt}S_t)_{t \geq 0}$ is a $\mathbb{Q}$-martingale.
\end{definition}

We will use the fact that $M_t = e^{c W_t - c^2t/2}$ is a martingale for any constant $c$, proven in example sheets.

In the Black-Scholes model,
\[
e^{-rt}S_t = S_0 e^{(\mu - r)t + \sigma W_t} = S_0 e^{(\mu - r)t + \sigma(\hat W_t + ct)}.
\]
Recall Cameron-Martin:
\[
\frac{\diff \mathbb{Q}}{\diff \mathbb{P}} = e^{c W_T - c^2T/2},
\]
then $\hat W_t = W_t - ct$ is a $\mathbb{Q}$-Brownian motion. To apply Cameron-Martin, we need $\mu -r + \sigma c = - \sigma^2/2$. Hence, we need
\[
c = \frac{r - \mu}{\sigma} - \frac{\sigma}{2}.
\]
Then,
\[
e^{-rt}S_t = S_0 e^{-\sigma^2t/2 + \sigma \hat W_t},
\]
so is a $\mathbb{Q}$-martingale. In fact, it is the unique risk-neutral measure.

\subsection{Black-Scholes Pricing}%
\label{sub:bs_price}

\begin{definition}
	The \emph{Black-Scholes price}\index{Black-Scholes price} of a European claim with payout $Y$ is
	\[
	\pi_t = e^{-(T - t)r} \mathbb{E}^{\mathbb{Q}}[Y \mid \mathcal{F}_t],
	\]
	where $T$ is the maturity date of the claim.
\end{definition}

\begin{remark}
	$e^{-rt}\pi_t = \mathbb{E}^{\mathbb{Q}}[e^{-rT}Y \mid \mathcal{F}_t]$ is a $\mathbb{Q}$-martingale.
\end{remark}

\begin{exbox}
	The Black-Scholes formula - this is the price of a European call:
	\begin{align*}
		\pi_t &= e^{-r(T-t)}\mathbb{E}^{\mathbb{Q}}[(S_T - K)^{+} \mid \mathcal{F}_t] \\
		      &= e^{-r(T - t)}\mathbb{E}^{\mathbb{Q}}[(S_t e^{(r - \sigma^2/2)(T - t) + \sigma(\hat W_T - \hat W_t)} - K)^{+} \mid \mathcal{F}_t] \\
		      &= S_t \mathbb{E}^{\mathbb{Q}}\left[\left(e^{-\sigma^2/2(T - t) + \sigma(\hat W_T - \hat W_t)} - \frac{K}{S_t} e^{-r(T - t)} \right)^{+} \bigm| \mathcal{F}_t\right].
	\end{align*}
	Now define
	\[
	F(v, m) = \mathbb{E}\left[(e^{-v/2 + \sqrt v Z} - M )^{+}\right],
	\]
	where $Z \sim N(0, 1)$. Then the Black-Scholes formula says
	\[
	\pi_t = S_t F\left( \sigma^2(T - t), \frac{K e^{-r(T - t)}}{S_t} \right).
	\]
	Now we simplify $F$. Note that
	\begin{align*}
		F &= \mathbb{E}\left[(e^{-v/2 + \sqrt v Z} - m ) \mathbbm{1}_{(Z \geq \log m/\sqrt v + \sqrt v /2)} \right] \\
		  &= \mathbb{E}\left[e^{-v/2 + \sqrt v Z} \mathbbm{1}_{Z \geq \log m / \sqrt v + \sqrt v / 2} \right] - m \mathbb{P}\left( Z > \frac{\log m}{\sqrt v} + \frac{\sqrt v}{2} \right) \\
		  &= \mathbb{P}\left( Z + \sqrt v \geq \frac{\log m}{\sqrt v} + \frac{\sqrt v}{2} \right) - m \mathbb{P} \left(Z > \frac{\log m}{\sqrt v} + \frac{\sqrt v}{2} \right).
	\end{align*}
	Let $\Phi$ be the CDF of the normal distribution. Note that $\mathbb{P}(Z > x) = \Phi(-x)$, by symmetry. Hence the Black-Scholes formula gives
	\begin{align*}
		\pi_t = S_t \Phi(d_1) - K e^{-r(T - t)} \Phi(d_2),
	\end{align*}
	where
	\begin{align*}
		d_1 &= \frac{- \log (Ke^{-r(T - t)}/S_t)}{\sigma \sqrt{T - t}} + \frac{\sigma \sqrt{T - t}}{2}, \\
		d_2 &= \frac{- \log (K e^{-r(T - t)}/S_t)}{\sigma \sqrt{T - t}} - \frac{\sigma \sqrt{T - t}}{2}.
	\end{align*}
	Here, everything is directly observable, apart from $\sigma$, the volatility.
\end{exbox}

For the European put, the payout is $(K - S_t)^{+}$. We could do this above derivation again, but instead we will use the following:

\emph{Put-call parity:}\index{put-call parity} Let $P_t$ be the price of a put at strike $K$, and expiry $T$, and $C_t$ the price of the call at the same strike $K$, and expiry $T$. Then,
\[\
	P_t - C_t = K e^{-r(T - t)} - S_t.
\]
To prove this, note that
\[
P_T - C_T = (K - S_T)^{+} - (S_T - K)^{+} = K - S_T.
\]
Hence,
\begin{align*}
	P_t - C_t &= e^{-r(T - t)} \mathbb{E}^{\mathbb{Q}}[P_T - S_T \mid \mathcal{F}_t] \\
		  &= K e^{-r(T - t)} - e^{-r(T - t)} \mathbb{E}^{\mathbb{Q}}[S_T \mid \mathcal{F}_t] \\
		  &= K e^{-r(T - t)} - S_t.
\end{align*}
Hence the Black-Scholes put price is
\begin{align*}
	S_t \Phi(d_1) &- K e^{-r(T _ t)}\Phi(d_2) + K e^{-r(T - t)} - S_t \\
		      &= Ke^{-r(T - t)} \Phi(-d_2) - S_t \Phi(-d_1).
\end{align*}

The general formula for the price of a vanilla claim with payout $Y = g(S_T)$ is
\begin{align*}
	\pi_t &= e^{-r(T - t)} \mathbb{E}^{\mathbb{Q}}[g(S_T) \mid \mathcal{F}_t] = V(t, S_t),
\end{align*}
where
\[
	V(t, s) = e^{-r(T - t)} \mathbb{E}[g(s e^{(r - \sigma^2/2)(T -t) + \sigma\sqrt{T - t} Z}],
\]
where $Z \sim N(0, 1)$.

% lecture 24

\subsection{Black-Scholes PDE}%
\label{sub:bs_pde}

Note that this function $V$ satisfies the following:
\begin{align*}
	V(T, s) = g(s), \\
	\frac{\partial V}{\partial t} + r s \frac{\partial V}{\partial s} + \frac{1}{2} S^2\sigma^2 \frac{\partial^2 V}{\partial S^2} = rV,
\end{align*}
for all $t < T$. We will derive this. Think of a binomial model approximation, where
\[
	(1 + r \delta) V(t - \delta, s) = q V(t, s(1+b)) + (1 - q)V(t, s(1+a)).
\]
Taking the Taylor series ($\delta$ small) of the right side, we get this is
\[
qV(t, s) + q \frac{\partial V}{\partial s} s b + \frac{1}{2} q \frac{\partial^2 V}{\partial s^2} s^2 b^2 + (1 - q)V + (1 - q) \frac{\partial V}{\partial s} sa + (1 - q) \frac{\partial^2 V}{\partial s^2} s^2 a^2,
\]
plus higher order terms. For the left hand side, the Taylor expansion is
\[
V(t, s) + \left(rV - \frac{\partial V}{\partial t} \right) \delta,
\]
plus higher order terms. Now notice that $qb + (1 - q)a = r \delta$, and we can call $\delta\sigma^2 = q b^2 + (1 - q)a^2$ to rearrange.

Another derivation is as follows. Let
\[
u(\tau, x) = \mathbb{E}[f(x + \sqrt \tau Z)],
\]
where $Z \sim N(0, 1)$. Then we can prove that
\[
\frac{\partial u}{\partial \tau} = \frac{1}{2} \frac{\tau^2 u}{\partial x^2}.
\]

\begin{proofbox}
	Note that
	\[
	u(\tau, x) = \int f(y) p(\tau, x, y) \diff y,
	\]
	where
	\[
		p(\tau, x, y) = \frac{1}{\sqrt{2 \pi \tau}}e^{-(x-  y)^2/2\tau},
	\]
	and we can then show that
	\[
	\frac{\partial p}{\partial \tau}  = \frac{1}{2} \frac{\partial^2 p}{\partial x^2},
	\]
	by algebra.
\end{proofbox}

\subsection{Hedging Contingent Claims and Greeks}%
\label{sub:hedge_greeks}

In the binomial model, to hedge a vanilla claim, you must hold
\[
\frac{V(n, S_{n-1}(1 + b)) - V(n, S_{n-1}(1+a))}{S_{n-1}(b - a)}
\]
shares at time $n$. In Black-Scholes, the answer is to hold
\[
\frac{\partial V}{\partial S}(t, S_t).
\]

\begin{definition}
	The \emph{Delta}\index{delta} is $\partial V/\partial S$; this is the sensitivity of the claim price with respect to the underlying asset price.

	The \emph{Gamma}\index{gamma} is $\partial^2 V/\partial S^2$.

	The \emph{Theta}\index{theta} is $\partial V/ \partial t$.
\end{definition}

The Black-Scholes PDE says that
\[
	\text{Theta} + rS \times \text{Delta} + \frac{1}{2} S^2 \sigma^2 \times \text{Gamma} = r \times \text{Price}.
\]

\begin{proposition}
	Suppose $g$ is increasing. Then the delta is non-negative. If $g$ is convex, then gamma is non-negative.
\end{proposition}

\begin{proofbox}
	\[
		V(t, s) = e^{-r(T - t)} \mathbb{E}[g(s e^{(r - \sigma^2/2)(T - t) + \sqrt{T - t} \sigma Z})].
	\]
	We now use the technique of example sheet 1, so if $g$ is increasing the $V(t, \cdot)$ is increasing.
\end{proofbox}

\subsection{Barrier Claims}%
\label{sub:barrier}

To a vanilla claim, we can modify it by saying it is:
\begin{itemize}
	\item down and in,
	\item down and out,
	\item up and in,
	\item up and out.
\end{itemize}

\begin{exbox}
	For example and up-and-in claim of the payout $g(S_T)$ and barrier $B$ has pay
	\[
	g(S_T) \mathbbm{1}_{(\max_{0 \leq t \leq T} S_T \geq B)}.
	\]
	An up-and-out claim of the payout has payout
	\[
	g(S_T) \mathbbm{1}_{(\max_{0 \leq t \leq T} S_t < B)}.
	\]
\end{exbox}

\begin{proposition}
	The initial price of an up-and-out claim with payout
	\[
	g(S_T) \mathbbm{1}_{(\max_{0 \leq t \leq T}S_t < B)}
	\]
	is the same as the initial price of a vanilla claim with payout
	\[
	g(S_T) \mathbbm{1}_{(S_T < B)} - \left( \frac{B}{S_0} \right)^{2r/\sigma^2 - 1} g\left( \frac{B^2 S_T}{S_0^2} \right) \mathbbm{1}_{(S_T \leq S_0^2/B)}.
	\]
\end{proposition}

\begin{proofbox}
	Note that
	\[
	\mathbbm{1}_{(\max S_t < B)} = \mathbbm{1}_{(S_T < B)} - \mathbbm{1}_{(\max S_T \geq B, S_T < B)},
	\]
	and the first term is accounted for. The second term has price
	\begin{align*}
		\mathbb{E}[g(S_T) \mathbbm{1}_{(\max S_t \geq B, S_T < B)}] = \mathbb{E}[g(S_0 e^{\sigma(W_T + cT)}) \mathbbm{1}_{(\max(W_t + ct) \geq b, W_T + cT \leq b}],
	\end{align*}
	where $c = (r - \sigma^2/2)/\sigma$, $W$ is a Brownian motion under $\mathbb{Q}$, and $b = \log(B/S_0)/\sigma$. Then by Cameron-Martin, this is
	\begin{align*}
		\mathbb{E}[e^{cW_T - c^2T/2} &g(S_0 e^{\sigma W_T}) \mathbbm{1}_{(\max _{0 \leq t \leq T} W_t \geq b, W_T \leq b)}] \\
					     &=  \mathbb{E}[e^{c(2b - W_T) - c^2T/2}g(S_0 e^{\sigma(2b - W_T)} \mathbbm{1}_{(W_T \geq b)}] \\
					     &= \mathbb{E}[e^{c(2b + W_T) - c^2T/2}g(S_0 e^{\sigma(2b + W_T)}) \mathbbm{1}_{(W_T \leq -b)}] \\
					     &=  e^{2c b} \mathbb{E}[g(S_0 e^{2b\sigma} e^{\sigma(W_T + cT)}) \mathbbm{1}_{(W_T + cT \leq -b)}].
	\end{align*}
	Now note that
	\[
	e^{2c b} = \left( \frac{B}{S_0} \right)^{2r/\sigma^2 - 1},
	\]
	and remember that $S_T = S_0 e^{\sigma(W_T + c T)}$.
\end{proofbox}


\newpage

\printindex

\end{document}
